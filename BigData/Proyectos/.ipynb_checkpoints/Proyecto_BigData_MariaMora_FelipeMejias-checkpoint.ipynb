{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KoVzbpenBYwm"
   },
   "source": [
    "# Big Data\n",
    "# Proyecto Programado\n",
    "\n",
    "- Professor: Luis Chavarría.\n",
    "\n",
    "- Student:  \n",
    "    - Felipe Alberto Mejías Loría, Instituto Tecnológico de Costa Rica.\n",
    "    - María Mora, Instituto Tecnológico de Costa Rica.\n",
    "\n",
    "- January 21th, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16-eehY1yTlt"
   },
   "source": [
    "## **1-) Instalación de PySpark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yfvwXXdbU-vR"
   },
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "import findspark\n",
    "\n",
    "# Set SPARK_HOME. Needed to initialize Apache Spark.\n",
    "SPARK_PATH = 'C:\\Users\\mejiasf\\Desktop\\Spark\\spark-2.4.4-bin-hadoop2.7'\n",
    "findspark.init(SPARK_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "etOFvKZQy3vz"
   },
   "source": [
    "# **2-) Importar bibliotecas necesarias para la ejecución del proyecto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dr7X3A0ezHGG"
   },
   "outputs": [],
   "source": [
    "# Necessary Imports for the execution of the TP3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import findspark\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession, Row, dataframe\n",
    "from pyspark.sql.functions import col, date_format, udf, array, explode, trim, lower, ltrim, rtrim\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.types import (StringType, IntegerType, FloatType, \n",
    "                               DecimalType, StructField, StructType)\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.linalg import DenseVector, SparseVector, Vectors, VectorUDT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QYcOWAaFfui0"
   },
   "source": [
    "# **3-) Funciones utilitarias para la construcción de DataFrames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LsVJnOhA0PXN"
   },
   "outputs": [],
   "source": [
    "POSTGRESQL_URL = \"jdbc:postgresql://localhost/\"\n",
    "POSTGRESQL_USER = \"postgres\"\n",
    "POSTGRESQL_PASSWORD = \"big_data\"\n",
    "\n",
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    This function builds a Spark Session\n",
    "    return the main entry of a Spark DataFrame\n",
    "    \"\"\"\n",
    "    spark = SparkSession \\\n",
    "      .builder \\\n",
    "      .appName(\"Basic JDBC pipeline\") \\\n",
    "      .config(\"spark.driver.extraClassPath\", \"postgresql-42.1.4.jar\") \\\n",
    "      .config(\"spark.executor.extraClassPath\", \"postgresql-42.1.4.jar\") \\\n",
    "      .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "def join_spark_data_frames(data_frame_1, data_frame_2,\n",
    "                           using_column_data_frame_1,\n",
    "                           using_column_data_frame_2):\n",
    "    \"\"\"\n",
    "    This function joint two Spark Data Frames\n",
    "    data_frame_1: Spark DataFrame 1\n",
    "    data_frame_2: Spark DataFrame 2\n",
    "    using_column_data_frame_1: Column from DataFrame 1 to compare\n",
    "    using_column_data_frame_2: Column from DataFrame 2 to compare\n",
    "    return the Spark DataFrame from the JOIN\n",
    "    \"\"\"\n",
    "    using_columns_statement = using_column_data_frame_1 == using_column_data_frame_2\n",
    "    joint_data_frame = data_frame_1.join(data_frame_2, using_columns_statement)\n",
    "\n",
    "    # To remove duplicated columns\n",
    "    joint_data_frame = joint_data_frame.drop(using_column_data_frame_1)\n",
    "\n",
    "    return joint_data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4-) Datos de entrada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_entry_data_description():\n",
    "    \"\"\"\n",
    "    This function shows a description of all the entry data columns\n",
    "    \"\"\"\n",
    "    \n",
    "    # OIJ Dataset Explanation\n",
    "    print(\"\\na-) The first dataset contains information taken from the OIJ Police Statistics of Costa Rica.\")\n",
    "    \n",
    "    print(\"\\nb-) Columns description for OIJ dataset: \\n\")\n",
    "    print(\"Delito: Descripción del Delito\")\n",
    "    print(\"SubDelito: Descripción del SubDelito\")\n",
    "    print(\"Fecha: Fecha del Hecho\")\n",
    "    print(\"Hora: Rango de 3 horas del Hecho\")\n",
    "    print(\"Victima: Descripción de la Víctima \")\n",
    "    print(\"SubVictima: Descripción de la SubVíctima\")\n",
    "    print(\"Edad: Grupo de Edad que pertenece la Víctima\")\n",
    "    print(\"Genero: Género de la Víctima\")\n",
    "    print(\"Nacionalidad: Nacionalidad de la Víctima\")\n",
    "    print(\"Provincia: Provincia del Lugar del Hecho\")\n",
    "    print(\"Canton: Cantón del Lugar del Hecho\")\n",
    "    print(\"Distrito: Distrito del Lugar del Hecho\")\n",
    "    \n",
    "    # INEC Dataset Explanation\n",
    "    print(\"\\nc-) The second dataset contains information about Economic Indicators according to province, canton\")\n",
    "    print(\"    and district taken from INEC.\")\n",
    "    \n",
    "    print(\"\\nb-) Columns description for INEC dataset: \\n\")\n",
    "    print(\"Columna 1: Provincia, Cantón y Distrito\")\n",
    "    print(\"Columna 2: Población de 15 años y más\")\n",
    "    print(\"Columna 3: Tasa neta de participación\")\n",
    "    print(\"Columna 4: Tasa de ocupación\")\n",
    "    print(\"Columna 5: Tasa de desempleo abierto\")\n",
    "    print(\"Columna 6: Porcentaje de poblacion economicamente inactiva\")\n",
    "    print(\"Columna 7: Relación de depedencia económica\")\n",
    "    \n",
    "    # Show which columns is going to be predicted\n",
    "    print(\"\\ne-) The column that is going to be predicted is the type of Delito in San Jose province according to the canton.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5-) Cargado y preprocesamiento de datos antes de cruzarlos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "OIJ_DATAFRAME_UNNECESSARY_COLUMNS = [\"Fecha\", \"Hora\", \"SubVictima\", \"Provincia\"]\n",
    "INEC_DATAFRAME_UNNECESSARY_COLUMNS = [\"Porcentaje de poblacion economicamente inactiva\",\n",
    "                                      \"Relacion de dependencia economica\",\n",
    "                                      \"Porcentaje de poblacion ocupada - Sector Primario\",\n",
    "                                      \"Porcentaje de poblacion ocupada - Sector Secundario\",\n",
    "                                      \"Porcentaje de poblacion ocupada - Sector Terciario\"]\n",
    "\n",
    "def convert_categorical_values_to_numerical_from_df(spark_df, categorical_columns_list):\n",
    "    \"\"\"\n",
    "    This function creates a Spark DataFrame with all values as numerical\n",
    "    spark_df: Spark DataFrame\n",
    "    return the Spark DataFrame with all values as numerical\n",
    "    \"\"\"\n",
    "    # Convert categorical columns to numerical values\n",
    "    for categorical_column in categorical_columns_list:\n",
    "        new_column_name = \"{column_name}_index\".format(column_name = categorical_column)\n",
    "        indexer = StringIndexer(inputCol=categorical_column, outputCol=new_column_name)\n",
    "        spark_df = indexer.fit(spark_df).transform(spark_df)\n",
    "    \n",
    "    # Remove categorical columns\n",
    "    columns_to_drop = categorical_columns_list\n",
    "    spark_df = spark_df.drop(*columns_to_drop)\n",
    "    \n",
    "    # Rename new numerical columns\n",
    "    for categorical_column in categorical_columns_list:\n",
    "        new_column_name = \"{column_name}_index\".format(column_name = categorical_column)\n",
    "        spark_df = spark_df.withColumnRenamed(new_column_name, categorical_column)\n",
    "    \n",
    "    # Show converted data\n",
    "    print(\"- Show that the data has been converted successfully from categorical to numerical: \\n\")\n",
    "    spark_df.select(categorical_columns_list).show()\n",
    "\n",
    "    return spark_df\n",
    "\n",
    "def rename_oij_spark_dataframe_columns(spark_df):\n",
    "    \"\"\"\n",
    "    This function is necessary as OIJ Dataset is outdated since the CSV\n",
    "    they provide does not bring the time field\n",
    "    spark_df: Spark DataFrame\n",
    "    return the Spark DataFrame with right columns names\n",
    "    \"\"\"\n",
    "    spark_df = spark_df.withColumnRenamed('Victima', 'Hora')\n",
    "    spark_df = spark_df.withColumnRenamed('SubVictima', 'Victima')\n",
    "    spark_df = spark_df.withColumnRenamed('Edad', 'SubVictima')\n",
    "    spark_df = spark_df.withColumnRenamed('Genero', 'Edad')\n",
    "    spark_df = spark_df.withColumnRenamed('Nacionalidad', 'Genero')\n",
    "    spark_df = spark_df.withColumnRenamed('Provincia', 'Nacionalidad')\n",
    "    spark_df = spark_df.withColumnRenamed('Canton', 'Provincia')\n",
    "    spark_df = spark_df.withColumnRenamed('Distrito', 'Canton')\n",
    "    return spark_df\n",
    "\n",
    "def remove_spark_dataframe_trailing_whitespaces(spark_df):\n",
    "    \"\"\"\n",
    "    This function removes all trailing whitespaces in Spark DataFrame Columns\n",
    "    spark_df: Spark DataFrame\n",
    "    return the Spark DataFrame\n",
    "    \"\"\"\n",
    "    for column in spark_df.columns:\n",
    "        spark_df = spark_df.withColumn(column, trim(col(column)).cast(spark_df.schema[column].dataType))\n",
    "    return spark_df\n",
    "\n",
    "def convert_spark_dataframe_to_lower_case(spark_df):\n",
    "    \"\"\"\n",
    "    This function converts to lower case Spark DataFrame Columns\n",
    "    spark_df: Spark DataFrame\n",
    "    columns_list: Columns List\n",
    "    return the Spark DataFrame\n",
    "    \"\"\"\n",
    "    for column in spark_df.columns:\n",
    "        spark_df =  spark_df.withColumn(column, lower(col(column)).cast(spark_df.schema[column].dataType))\n",
    "    return spark_df\n",
    "    \n",
    "def create_and_clean_spark_dataframe_from_csv(spark_session,\n",
    "                                              csv_file_name,\n",
    "                                              schema_types_list):\n",
    "    \"\"\"\n",
    "    This function creates a Spark DataFrame from Dataset CSV file\n",
    "    spark_session: Spark Session\n",
    "    return the Spark DataFrame\n",
    "    \"\"\"\n",
    "    print(\"\\n- Loading and cleaning CSV input file: {name}\".format(name = csv_file_name))\n",
    "    \n",
    "    spark_df = spark_session \\\n",
    "      .read \\\n",
    "      .format(\"csv\") \\\n",
    "      .option(\"path\", csv_file_name) \\\n",
    "      .option(\"header\", True) \\\n",
    "      .schema(StructType(schema_types_list)) \\\n",
    "      .load()\n",
    "    \n",
    "    # Rename OIJ Spark DataFrame Columns\n",
    "    spark_df = rename_oij_spark_dataframe_columns(spark_df) \\\n",
    "               if csv_file_name == \"datos_delitos_oij_2011.csv\" \\\n",
    "               else spark_df\n",
    "    \n",
    "    # Clean whitespaces\n",
    "    spark_df = remove_spark_dataframe_trailing_whitespaces(spark_df)\n",
    "    \n",
    "    # Convert to lower case\n",
    "    spark_df = convert_spark_dataframe_to_lower_case(spark_df)\n",
    "    \n",
    "    return spark_df\n",
    "\n",
    "def remove_unnecessary_columns(spark_df, columns_to_remove_list):\n",
    "    \"\"\"\n",
    "    This function removes unnecessary columns for DataFrame\n",
    "    \"\"\"\n",
    "    columns_to_drop = columns_to_remove_list\n",
    "    spark_df = spark_df.drop(*columns_to_drop)\n",
    "    return spark_df\n",
    "\n",
    "def preprocessing_oij_data(spark_session):\n",
    "    \"\"\"\n",
    "    This function completes data preprocessing before training model\n",
    "    for OIJ Dataset\n",
    "    \"\"\"\n",
    "    schema_types_list = [StructField(\"Delito\", StringType()),\n",
    "                         StructField(\"SubDelito\", StringType()),\n",
    "                         StructField(\"Fecha\", StringType()),\n",
    "                         StructField(\"Victima\", StringType()),\n",
    "                         StructField(\"SubVictima\", StringType()),\n",
    "                         StructField(\"Edad\", StringType()),\n",
    "                         StructField(\"Genero\", StringType()),\n",
    "                         StructField(\"Nacionalidad\", StringType()),\n",
    "                         StructField(\"Provincia\", StringType()),\n",
    "                         StructField(\"Canton\", StringType()),\n",
    "                         StructField(\"Distrito\", StringType())]\n",
    "    \n",
    "    oij_spark_df = create_and_clean_spark_dataframe_from_csv(spark_session,\n",
    "                                                             \"datos_delitos_oij_2011.csv\",\n",
    "                                                             schema_types_list)\n",
    "    \n",
    "    oij_spark_df = remove_unnecessary_columns(oij_spark_df, OIJ_DATAFRAME_UNNECESSARY_COLUMNS)\n",
    "    \n",
    "    return oij_spark_df\n",
    "\n",
    "def preprocessing_inec_data(spark_session):\n",
    "    \"\"\"\n",
    "    This function completes data preprocessing before training model\n",
    "    for INEC Dataset\n",
    "    \"\"\"\n",
    "    schema_types_list = [StructField(\"Canton\", StringType()),\n",
    "                         StructField(\"Poblacion de 15 anos y mas\", IntegerType()),\n",
    "                         StructField(\"Tasa neta de participacion\", FloatType()),\n",
    "                         StructField(\"Tasa de ocupacion\", FloatType()),\n",
    "                         StructField(\"Tasa de desempleo abierto\", FloatType()),\n",
    "                         StructField(\"Porcentaje de poblacion economicamente inactiva\", FloatType()),\n",
    "                         StructField(\"Relacion de dependencia economica\", FloatType()),\n",
    "                         StructField(\"Porcentaje de poblacion ocupada - Sector Primario\", FloatType()),\n",
    "                         StructField(\"Porcentaje de poblacion ocupada - Sector Secundario\", FloatType()),\n",
    "                         StructField(\"Porcentaje de poblacion ocupada - Sector Terciario\", FloatType())]\n",
    "    \n",
    "    inec_spark_df = create_and_clean_spark_dataframe_from_csv(spark_session,\n",
    "                                                             \"datos_socioeconomicos_inec_2011.csv\",\n",
    "                                                             schema_types_list)\n",
    "    \n",
    "    inec_spark_df = remove_unnecessary_columns(inec_spark_df, INEC_DATAFRAME_UNNECESSARY_COLUMNS)\n",
    "    \n",
    "    return inec_spark_df\n",
    "\n",
    "def show_preprocessing_spark_dataframe(spark_df, columns_list):\n",
    "    \"\"\"\n",
    "    This function completes data preprocessing before training model\n",
    "    \"\"\"\n",
    "    print(\"\\n1-) Definition of schema: \\n\")\n",
    "    spark_df.printSchema()\n",
    "    \n",
    "    print(\"2-) Show that the data has been loaded successfully by selecting a couple of rows: \\n\")\n",
    "    spark_df.select(columns_list).show()\n",
    "    \n",
    "\n",
    "def data_preprocessing(spark_session):\n",
    "    \"\"\"\n",
    "    This function completes data preprocessing for INEC and OIJ before training model\n",
    "    \"\"\"\n",
    "    # Loading and cleaning OIJ CSV input file data.\n",
    "    oij_spark_df = preprocessing_oij_data(spark_session)\n",
    "    \n",
    "    # Show Preprocessing OIJ DataFrame\n",
    "    show_preprocessing_spark_dataframe(oij_spark_df,\n",
    "                                       [\"Delito\", \"SubDelito\", \"Genero\", \"Canton\"])\n",
    "    \n",
    "    # Convert all OIJ Categorical Values to Numerical Values\n",
    "    oij_spark_df = convert_categorical_values_to_numerical_from_df(oij_spark_df,\n",
    "                                                                   [\"Delito\", \"SubDelito\",\n",
    "                                                                    \"Victima\", \"Edad\", \"Genero\",\n",
    "                                                                    \"Nacionalidad\"])\n",
    "    \n",
    "    # Loading and cleaning INEC CSV input file data.\n",
    "    inec_spark_df = preprocessing_inec_data(spark_session)\n",
    "    \n",
    "    # Show Preprocessing INEC DataFrame\n",
    "    show_preprocessing_spark_dataframe(inec_spark_df,\n",
    "                                       [\"Canton\", \"Poblacion de 15 anos y mas\", \"Tasa de desempleo abierto\"])\n",
    "    \n",
    "    return oij_spark_df, inec_spark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6-) Materialización en Postgresql**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_LIST = ['SubDelito', 'Victima', 'Edad',\n",
    "                 'Genero', 'Nacionalidad',\n",
    "                 'Poblacion de 15 anos y mas', 'Tasa neta de participacion',\n",
    "                 'Tasa de ocupacion', 'Tasa de desempleo abierto', 'Canton', 'Delito']\n",
    "\n",
    "def create_joint_spark_data_frames(oij_data_frame, inec_data_frame):\n",
    "    \"\"\"\n",
    "    This function creates the data frame of the joint of the two datasets\n",
    "    oij_data_frame: DataFrame with the students info\n",
    "    inec_data_frame: DataFrame with the courses info\n",
    "    return the joint Spark DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nThe data union between OIJ DataFrame and INEC DataFrame is the following: \\n\")\n",
    "    joint_oij_and_inec_df = join_spark_data_frames(oij_data_frame,\n",
    "                                                   inec_data_frame,\n",
    "                                                   oij_data_frame.Canton,\n",
    "                                                   inec_data_frame.Canton)\n",
    "    \n",
    "    joint_oij_and_inec_df.select([\"Delito\", \"Genero\", \"Canton\",\n",
    "                                  \"Poblacion de 15 anos y mas\",\n",
    "                                  \"Tasa de desempleo abierto\",\n",
    "                                  \"Tasa de ocupacion\"]).show(20)\n",
    "\n",
    "    return joint_oij_and_inec_df\n",
    "\n",
    "def transform_sparse_vector_df_to_dense_vector_df(spark_df):\n",
    "    \"\"\"\n",
    "    This function transforms a sparse vector to a dense vector df\n",
    "    \"\"\"\n",
    "    toDense = lambda v: Vectors.dense(v.toArray())\n",
    "    toDenseUdf = udf(toDense, VectorUDT())\n",
    "    spark_df = spark_df.withColumn('features', toDenseUdf('features'))\n",
    "    return spark_df\n",
    "\n",
    "def transform_spark_df_to_features_vector_df(spark_df, remove_delito=False):\n",
    "    \"\"\"\n",
    "    This function transforms a spark df to a features vector df\n",
    "    \"\"\"\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=FEATURES_LIST[:-1] if remove_delito else \\\n",
    "                  FEATURES_LIST,\n",
    "        outputCol='features')\n",
    "    \n",
    "    vector_df = assembler.transform(spark_df)\n",
    "    vector_df = vector_df.select(['features', 'Delito'])\n",
    "    \n",
    "    # Converte Sparse Vectors to Dense Vectors\n",
    "    vector_df = transform_sparse_vector_df_to_dense_vector_df(vector_df)\n",
    "\n",
    "    return vector_df\n",
    "\n",
    "def normalize_data(spark_df):\n",
    "    \"\"\"\n",
    "    This function normalize data before training the model\n",
    "    \"\"\"\n",
    "    vector_df = transform_spark_df_to_features_vector_df(spark_df, remove_delito=True)\n",
    "    standard_scaler = StandardScaler(inputCol='features', outputCol='scaled_features')\n",
    "    scale_model = standard_scaler.fit(vector_df)\n",
    "    scaled_df = scale_model.transform(vector_df)\n",
    "    \n",
    "    return scaled_df\n",
    "\n",
    "def convert_normalized_df_to_valid_table_for_db(normalized_df):\n",
    "    \"\"\"\n",
    "    This function converts normalize features vector into a valid DF for DB\n",
    "    \"\"\"\n",
    "    normalized_valid_db_format_df = normalized_df.select('scaled_features', 'Delito')\n",
    "    normalized_valid_db_format_df = normalized_valid_db_format_df.rdd.map(lambda x:[float(y) for y in x['scaled_features']]+[x['Delito']]).toDF(FEATURES_LIST)\n",
    "    \n",
    "    return normalized_valid_db_format_df\n",
    "\n",
    "def materialization_to_postgresql(oij_spark_df, inec_spark_df):\n",
    "    \"\"\"\n",
    "    This function materialize data to write to DB\n",
    "    \"\"\"\n",
    "    # Joint Spark Data Frames\n",
    "    joint_data_frame = create_joint_spark_data_frames(oij_spark_df,\n",
    "                                                      inec_spark_df)\n",
    "    \n",
    "    # Convert Canton Categorical Value to Numerical Values\n",
    "    joint_data_frame = convert_categorical_values_to_numerical_from_df(joint_data_frame,\n",
    "                                                                       [\"Canton\"])\n",
    "    \n",
    "    # Normalizing the data\n",
    "    print(\"\\n- After normalizing the data, the DataFrame looks like this: \\n\")\n",
    "    normalized_df = normalize_data(joint_data_frame)\n",
    "    normalized_df.show()\n",
    "    \n",
    "    # Convert to valid format for DB\n",
    "    print(\"\\ng-) Writing to DB: \")\n",
    "    print(\"\\ng.1-) Before writing the normalized Dataframe, is necessary to convert it to a valid table for DB: \")\n",
    "    normalized_valid_db_df = convert_normalized_df_to_valid_table_for_db(normalized_df)\n",
    "    print(\"- After convert it to a valid table for DB, the DataFrame looks like this: \\n\")\n",
    "    normalized_valid_db_df.show()\n",
    "    print(\"- In order to check that this table was written correctly into DB, you can go to PostgreSQL and check the table named tarea3.\")\n",
    "    print(\"  It should look exactly the same.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7-) Modelos de predicción**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset_from_db(spark_session):\n",
    "    \"\"\"\n",
    "    This function reads the clean dataset from the database\n",
    "    \"\"\"\n",
    "    spark_df = spark_session \\\n",
    "               .read \\\n",
    "               .format(\"jdbc\") \\\n",
    "               .option(\"url\", POSTGRESQL_URL) \\\n",
    "               .option(\"user\", POSTGRESQL_USER) \\\n",
    "               .option(\"password\", POSTGRESQL_PASSWORD) \\\n",
    "               .option(\"dbtable\", \"tarea3\") \\\n",
    "               .load()\n",
    "    spark_df.show()\n",
    "    return spark_df\n",
    "\n",
    "def split_train_test_data(spark_df):\n",
    "    \"\"\"\n",
    "    This function splits train and test data\n",
    "    \"\"\"\n",
    "    train, test = spark_df.randomSplit([0.8, 0.2], seed=12345)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Qz_1dBJJgFd"
   },
   "source": [
    "# **8-) Función main() para ejecutar el programa principal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "JSp6jIOgBXzs",
    "outputId": "ec260664-da48-425d-ad14-307d7e483b9d",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Entry Data Description\n",
      "\n",
      "a-) The first dataset contains information taken from the OIJ Police Statistics of Costa Rica.\n",
      "\n",
      "b-) Columns description for OIJ dataset: \n",
      "\n",
      "Delito: Descripción del Delito\n",
      "SubDelito: Descripción del SubDelito\n",
      "Fecha: Fecha del Hecho\n",
      "Hora: Rango de 3 horas del Hecho\n",
      "Victima: Descripción de la Víctima \n",
      "SubVictima: Descripción de la SubVíctima\n",
      "Edad: Grupo de Edad que pertenece la Víctima\n",
      "Genero: Género de la Víctima\n",
      "Nacionalidad: Nacionalidad de la Víctima\n",
      "Provincia: Provincia del Lugar del Hecho\n",
      "Canton: Cantón del Lugar del Hecho\n",
      "Distrito: Distrito del Lugar del Hecho\n",
      "\n",
      "c-) The second dataset contains information about Economic Indicators according to province, canton\n",
      "    and district taken from INEC.\n",
      "\n",
      "b-) Columns description for INEC dataset: \n",
      "\n",
      "Columna 1: Provincia, Cantón y Distrito\n",
      "Columna 2: Población de 15 años y más\n",
      "Columna 3: Tasa neta de participación\n",
      "Columna 4: Tasa de ocupación\n",
      "Columna 5: Tasa de desempleo abierto\n",
      "Columna 6: Porcentaje de poblacion economicamente inactiva\n",
      "Columna 7: Relación de depedencia económica\n",
      "\n",
      "e-) The column that is going to be predicted is the type of Delito in San Jose province according to the canton.\n",
      "\n",
      "Step 2: Data Preprocessing\n",
      "\n",
      "- Loading and cleaning CSV input file: datos_delitos_oij_2011.csv\n",
      "\n",
      "1-) Definition of schema: \n",
      "\n",
      "root\n",
      " |-- Delito: string (nullable = true)\n",
      " |-- SubDelito: string (nullable = true)\n",
      " |-- Victima: string (nullable = true)\n",
      " |-- Edad: string (nullable = true)\n",
      " |-- Genero: string (nullable = true)\n",
      " |-- Nacionalidad: string (nullable = true)\n",
      " |-- Canton: string (nullable = true)\n",
      "\n",
      "2-) Show that the data has been loaded successfully by selecting a couple of rows: \n",
      "\n",
      "+------+-----------+------+-------------------+\n",
      "|Delito|  SubDelito|Genero|             Canton|\n",
      "+------+-----------+------+-------------------+\n",
      "|asalto|arma blanca|hombre|      perez zeledon|\n",
      "|asalto|arma blanca| mujer|           san jose|\n",
      "|asalto|arma blanca|hombre|           san jose|\n",
      "|asalto|arma blanca|hombre|           san jose|\n",
      "|asalto|arma blanca|hombre|         curridabat|\n",
      "|asalto|arma blanca|hombre|           san jose|\n",
      "|asalto|arma blanca| mujer|      montes de oca|\n",
      "|asalto|arma blanca|hombre|      perez zeledon|\n",
      "|asalto|arma blanca|hombre|vasquez de coronado|\n",
      "|asalto|arma blanca|hombre|           san jose|\n",
      "|asalto|arma blanca|hombre|      perez zeledon|\n",
      "|asalto|arma blanca|hombre|            moravia|\n",
      "|asalto|arma blanca| mujer|      montes de oca|\n",
      "|asalto|arma blanca| mujer|       desamparados|\n",
      "|asalto|arma blanca|hombre|       desamparados|\n",
      "|asalto|arma blanca| mujer|      montes de oca|\n",
      "|asalto|arma blanca|hombre|       desamparados|\n",
      "|asalto|arma blanca|hombre|           san jose|\n",
      "|asalto|arma blanca|hombre|         goicoechea|\n",
      "|asalto|arma blanca|hombre|         curridabat|\n",
      "+------+-----------+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "- Show that the data has been converted successfully from categorical to numerical: \n",
      "\n",
      "+------+---------+-------+----+------+------------+\n",
      "|Delito|SubDelito|Victima|Edad|Genero|Nacionalidad|\n",
      "+------+---------+-------+----+------+------------+\n",
      "|   0.0|      1.0|    0.0| 1.0|   0.0|         0.0|\n",
      "|   0.0|      1.0|    0.0| 1.0|   1.0|         1.0|\n",
      "|   0.0|      1.0|    0.0| 0.0|   0.0|         0.0|\n",
      "|   0.0|      1.0|    0.0| 0.0|   0.0|        24.0|\n",
      "|   0.0|      1.0|    0.0| 0.0|   0.0|         0.0|\n",
      "|   0.0|      1.0|    0.0| 0.0|   0.0|         0.0|\n",
      "|   0.0|      1.0|    0.0| 0.0|   1.0|         9.0|\n",
      "|   0.0|      1.0|    0.0| 0.0|   0.0|         0.0|\n",
      "|   0.0|      1.0|    0.0| 0.0|   0.0|         0.0|\n",
      "|   0.0|      1.0|    0.0| 0.0|   0.0|         0.0|\n",
      "|   0.0|      1.0|    0.0| 0.0|   0.0|         0.0|\n",
      "|   0.0|      1.0|    0.0| 1.0|   0.0|         0.0|\n",
      "|   0.0|      1.0|    0.0| 0.0|   1.0|         0.0|\n",
      "|   0.0|      1.0|    0.0| 0.0|   1.0|         0.0|\n",
      "|   0.0|      1.0|    0.0| 2.0|   0.0|         0.0|\n",
      "|   0.0|      1.0|    0.0| 0.0|   1.0|         0.0|\n",
      "|   0.0|      1.0|    0.0| 0.0|   0.0|         1.0|\n",
      "|   0.0|      1.0|    0.0| 0.0|   0.0|         0.0|\n",
      "|   0.0|      1.0|    0.0| 0.0|   0.0|         0.0|\n",
      "|   0.0|      1.0|    0.0| 1.0|   0.0|         0.0|\n",
      "+------+---------+-------+----+------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "- Loading and cleaning CSV input file: datos_socioeconomicos_inec_2011.csv\n",
      "\n",
      "1-) Definition of schema: \n",
      "\n",
      "root\n",
      " |-- Canton: string (nullable = true)\n",
      " |-- Poblacion de 15 anos y mas: integer (nullable = true)\n",
      " |-- Tasa neta de participacion: float (nullable = true)\n",
      " |-- Tasa de ocupacion: float (nullable = true)\n",
      " |-- Tasa de desempleo abierto: float (nullable = true)\n",
      "\n",
      "2-) Show that the data has been loaded successfully by selecting a couple of rows: \n",
      "\n",
      "+-------------------+--------------------------+-------------------------+\n",
      "|             Canton|Poblacion de 15 anos y mas|Tasa de desempleo abierto|\n",
      "+-------------------+--------------------------+-------------------------+\n",
      "|           san jose|                    225856|                      3.9|\n",
      "|             escazu|                     44797|                      3.0|\n",
      "|       desamparados|                    159292|                      4.0|\n",
      "|           puriscal|                     25774|                      3.1|\n",
      "|            tarrazu|                     11800|                      2.8|\n",
      "|             aserri|                     43396|                      3.2|\n",
      "|               mora|                     20414|                      3.2|\n",
      "|         goicoechea|                     90537|                      3.7|\n",
      "|          santa ana|                     38096|                      2.3|\n",
      "|         alajuelita|                     56704|                      3.8|\n",
      "|vazquez de coronado|                     47697|                      3.0|\n",
      "|             acosta|                     15270|                      2.1|\n",
      "|              tibas|                     52194|                      3.5|\n",
      "|            moravia|                     45444|                      3.2|\n",
      "|      montes de oca|                     41561|                      2.9|\n",
      "|         turrubares|                      4133|                      3.9|\n",
      "|               dota|                      5164|                      1.8|\n",
      "|         curridabat|                     51916|                      3.0|\n",
      "|      perez zeledon|                     98186|                      3.4|\n",
      "| leon cortes castro|                      9084|                      2.4|\n",
      "+-------------------+--------------------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Step 3: Materialization to PostgreSQL\n",
      "\n",
      "The data union between OIJ DataFrame and INEC DataFrame is the following: \n",
      "\n",
      "+------+------+-------------+--------------------------+-------------------------+-----------------+\n",
      "|Delito|Genero|       Canton|Poblacion de 15 anos y mas|Tasa de desempleo abierto|Tasa de ocupacion|\n",
      "+------+------+-------------+--------------------------+-------------------------+-----------------+\n",
      "|   0.0|   0.0|perez zeledon|                     98186|                      3.4|             46.4|\n",
      "|   0.0|   1.0|     san jose|                    225856|                      3.9|             54.5|\n",
      "|   0.0|   0.0|     san jose|                    225856|                      3.9|             54.5|\n",
      "|   0.0|   0.0|     san jose|                    225856|                      3.9|             54.5|\n",
      "|   0.0|   0.0|   curridabat|                     51916|                      3.0|             57.1|\n",
      "|   0.0|   0.0|     san jose|                    225856|                      3.9|             54.5|\n",
      "|   0.0|   1.0|montes de oca|                     41561|                      2.9|             57.1|\n",
      "|   0.0|   0.0|perez zeledon|                     98186|                      3.4|             46.4|\n",
      "|   0.0|   0.0|     san jose|                    225856|                      3.9|             54.5|\n",
      "|   0.0|   0.0|perez zeledon|                     98186|                      3.4|             46.4|\n",
      "|   0.0|   0.0|      moravia|                     45444|                      3.2|             56.8|\n",
      "|   0.0|   1.0|montes de oca|                     41561|                      2.9|             57.1|\n",
      "|   0.0|   1.0| desamparados|                    159292|                      4.0|             54.7|\n",
      "|   0.0|   0.0| desamparados|                    159292|                      4.0|             54.7|\n",
      "|   0.0|   1.0|montes de oca|                     41561|                      2.9|             57.1|\n",
      "|   0.0|   0.0| desamparados|                    159292|                      4.0|             54.7|\n",
      "|   0.0|   0.0|     san jose|                    225856|                      3.9|             54.5|\n",
      "|   0.0|   0.0|   goicoechea|                     90537|                      3.7|             54.7|\n",
      "|   0.0|   0.0|   curridabat|                     51916|                      3.0|             57.1|\n",
      "|   0.0|   1.0| desamparados|                    159292|                      4.0|             54.7|\n",
      "+------+------+-------------+--------------------------+-------------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Show that the data has been converted successfully from categorical to numerical: \n",
      "\n",
      "+------+\n",
      "|Canton|\n",
      "+------+\n",
      "|   3.0|\n",
      "|   0.0|\n",
      "|   0.0|\n",
      "|   0.0|\n",
      "|   5.0|\n",
      "|   0.0|\n",
      "|   2.0|\n",
      "|   3.0|\n",
      "|   0.0|\n",
      "|   3.0|\n",
      "|   9.0|\n",
      "|   2.0|\n",
      "|   1.0|\n",
      "|   1.0|\n",
      "|   2.0|\n",
      "|   1.0|\n",
      "|   0.0|\n",
      "|   4.0|\n",
      "|   5.0|\n",
      "|   1.0|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "- After normalizing the data, the DataFrame looks like this: \n",
      "\n",
      "+--------------------+------+--------------------+\n",
      "|            features|Delito|     scaled_features|\n",
      "+--------------------+------+--------------------+\n",
      "|[1.0,0.0,1.0,0.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,1.0,1.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,0.0,0.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,0.0,0.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,0.0,0.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,0.0,0.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,0.0,1.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,0.0,0.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,0.0,0.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,0.0,0.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,1.0,0.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,0.0,1.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,0.0,1.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,2.0,0.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,0.0,1.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,0.0,0.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,0.0,0.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,0.0,0.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,1.0,0.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,0.0,1.0,...|   0.0|[0.31499290187266...|\n",
      "+--------------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "g-) Writing to DB: \n",
      "\n",
      "g.1-) Before writing the normalized Dataframe, is necessary to convert it to a valid table for DB: \n",
      "- After convert it to a valid table for DB, the DataFrame looks like this: \n",
      "\n",
      "+------------------+-------+------------------+-----------------+-------------------+--------------------------+--------------------------+------------------+-------------------------+------------------+------+\n",
      "|         SubDelito|Victima|              Edad|           Genero|       Nacionalidad|Poblacion de 15 anos y mas|Tasa neta de participacion| Tasa de ocupacion|Tasa de desempleo abierto|            Canton|Delito|\n",
      "+------------------+-------+------------------+-----------------+-------------------+--------------------------+--------------------------+------------------+-------------------------+------------------+------+\n",
      "|0.3149929018726635|    0.0|2.4305252824998718|              0.0|                0.0|          1.25541377491375|         19.38275964575944| 19.11629268809971|        8.703035114014002|0.9842927375078734|   0.0|\n",
      "|0.3149929018726635|    0.0|2.4305252824998718|2.083182920207937|0.20442058775065666|         2.887812249678365|        22.895885139634007|22.453403388798698|        9.982893183117008|               0.0|   0.0|\n",
      "|0.3149929018726635|    0.0|               0.0|              0.0|                0.0|         2.887812249678365|        22.895885139634007|22.453403388798698|        9.982893183117008|               0.0|   0.0|\n",
      "|0.3149929018726635|    0.0|               0.0|              0.0|   4.90609410601576|         2.887812249678365|        22.895885139634007|22.453403388798698|        9.982893183117008|               0.0|   0.0|\n",
      "|0.3149929018726635|    0.0|               0.0|              0.0|                0.0|        0.6638019833624167|        23.784261931478646| 23.52457429796754|        7.679148414618043|1.6404878958464555|   0.0|\n",
      "|0.3149929018726635|    0.0|               0.0|              0.0|                0.0|         2.887812249678365|        22.895885139634007|22.453403388798698|        9.982893183117008|               0.0|   0.0|\n",
      "|0.3149929018726635|    0.0|               0.0|2.083182920207937|   1.83978528975591|        0.5314021540666731|        23.743880257974645| 23.52457429796754|        7.423177044910995|0.6561951583385822|   0.0|\n",
      "|0.3149929018726635|    0.0|               0.0|              0.0|                0.0|          1.25541377491375|         19.38275964575944| 19.11629268809971|        8.703035114014002|0.9842927375078734|   0.0|\n",
      "|0.3149929018726635|    0.0|               0.0|              0.0|                0.0|         2.887812249678365|        22.895885139634007|22.453403388798698|        9.982893183117008|               0.0|   0.0|\n",
      "|0.3149929018726635|    0.0|               0.0|              0.0|                0.0|          1.25541377491375|         19.38275964575944| 19.11629268809971|        8.703035114014002|0.9842927375078734|   0.0|\n",
      "|0.3149929018726635|    0.0|2.4305252824998718|              0.0|                0.0|        0.5810504917929282|        23.703500124873983|  23.4009778963886|        8.191091764316022|  2.95287821252362|   0.0|\n",
      "|0.3149929018726635|    0.0|               0.0|2.083182920207937|                0.0|        0.5314021540666731|        23.743880257974645| 23.52457429796754|        7.423177044910995|0.6561951583385822|   0.0|\n",
      "|0.3149929018726635|    0.0|               0.0|2.083182920207937|                0.0|         2.036719807646315|        23.017027079339336|22.535801513722497|       10.238864552824056|0.3280975791692911|   0.0|\n",
      "|0.3149929018726635|    0.0|4.8610505649997435|              0.0|                0.0|         2.036719807646315|        23.017027079339336|22.535801513722497|       10.238864552824056|0.3280975791692911|   0.0|\n",
      "|0.3149929018726635|    0.0|               0.0|2.083182920207937|                0.0|        0.5314021540666731|        23.743880257974645| 23.52457429796754|        7.423177044910995|0.6561951583385822|   0.0|\n",
      "|0.3149929018726635|    0.0|               0.0|              0.0|0.20442058775065666|         2.036719807646315|        23.017027079339336|22.535801513722497|       10.238864552824056|0.3280975791692911|   0.0|\n",
      "|0.3149929018726635|    0.0|               0.0|              0.0|                0.0|         2.887812249678365|        22.895885139634007|22.453403388798698|        9.982893183117008|               0.0|   0.0|\n",
      "|0.3149929018726635|    0.0|               0.0|              0.0|                0.0|         1.157613070492394|         22.93626527273467|22.535801513722497|        9.470949833419029|1.3123903166771644|   0.0|\n",
      "|0.3149929018726635|    0.0|2.4305252824998718|              0.0|                0.0|        0.6638019833624167|        23.784261931478646| 23.52457429796754|        7.679148414618043|1.6404878958464555|   0.0|\n",
      "|0.3149929018726635|    0.0|               0.0|2.083182920207937|                0.0|         2.036719807646315|        23.017027079339336|22.535801513722497|       10.238864552824056|0.3280975791692911|   0.0|\n",
      "+------------------+-------+------------------+-----------------+-------------------+--------------------------+--------------------------+------------------+-------------------------+------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "- In order to check that this table was written correctly into DB, you can go to PostgreSQL and check the table named tarea3.\n",
      "  It should look exactly the same.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    This function extract data and train a model using PySpark\n",
    "    \"\"\"\n",
    "    print(\"Step 1: Entry Data Description\")\n",
    "    show_entry_data_description()\n",
    "    \n",
    "    print(\"Step 2: Data Preprocessing\")\n",
    "    spark_session = create_spark_session()\n",
    "    oij_spark_df, inec_spark_df = data_preprocessing(spark_session)\n",
    "    \n",
    "    print(\"Step 3: Materialization to PostgreSQL\")\n",
    "    materialization_to_postgresql(oij_spark_df, inec_spark_df)\n",
    "\n",
    "# Execute main program\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "TP2_BigData_FelipeMejias.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
