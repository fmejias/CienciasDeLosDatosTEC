{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KoVzbpenBYwm"
   },
   "source": [
    "# Big Data\n",
    "# Proyecto Programado\n",
    "\n",
    "- Professor: Luis Chavarría.\n",
    "\n",
    "- Student:  \n",
    "    - Felipe Alberto Mejías Loría, Instituto Tecnológico de Costa Rica.\n",
    "    - María Mora, Instituto Tecnológico de Costa Rica.\n",
    "\n",
    "- January 21th, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16-eehY1yTlt"
   },
   "source": [
    "## **1-) Instalación de PySpark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yfvwXXdbU-vR"
   },
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "import findspark\n",
    "\n",
    "# Set SPARK_HOME. Needed to initialize Apache Spark.\n",
    "SPARK_PATH = 'C:\\Users\\mejiasf\\Desktop\\Spark\\spark-2.4.4-bin-hadoop2.7'\n",
    "findspark.init(SPARK_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "etOFvKZQy3vz"
   },
   "source": [
    "# **2-) Importar bibliotecas necesarias para la ejecución del proyecto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dr7X3A0ezHGG"
   },
   "outputs": [],
   "source": [
    "# Necessary Imports for the execution of the TP3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import findspark\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession, Row, dataframe\n",
    "from pyspark.sql.functions import col, date_format, udf, array, explode, trim, lower, ltrim, rtrim\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.types import (StringType, IntegerType, FloatType, \n",
    "                               DecimalType, StructField, StructType)\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.linalg import DenseVector, SparseVector, Vectors, VectorUDT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QYcOWAaFfui0"
   },
   "source": [
    "# **3-) Funciones utilitarias para la construcción de DataFrames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LsVJnOhA0PXN"
   },
   "outputs": [],
   "source": [
    "POSTGRESQL_URL = \"jdbc:postgresql://localhost/\"\n",
    "POSTGRESQL_USER = \"postgres\"\n",
    "POSTGRESQL_PASSWORD = \"big_data\"\n",
    "\n",
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    This function builds a Spark Session\n",
    "    return the main entry of a Spark DataFrame\n",
    "    \"\"\"\n",
    "    spark = SparkSession \\\n",
    "      .builder \\\n",
    "      .appName(\"Basic JDBC pipeline\") \\\n",
    "      .config(\"spark.driver.extraClassPath\", \"postgresql-42.1.4.jar\") \\\n",
    "      .config(\"spark.executor.extraClassPath\", \"postgresql-42.1.4.jar\") \\\n",
    "      .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "def join_spark_data_frames(data_frame_1, data_frame_2,\n",
    "                           using_column_data_frame_1,\n",
    "                           using_column_data_frame_2):\n",
    "    \"\"\"\n",
    "    This function joint two Spark Data Frames\n",
    "    data_frame_1: Spark DataFrame 1\n",
    "    data_frame_2: Spark DataFrame 2\n",
    "    using_column_data_frame_1: Column from DataFrame 1 to compare\n",
    "    using_column_data_frame_2: Column from DataFrame 2 to compare\n",
    "    return the Spark DataFrame from the JOIN\n",
    "    \"\"\"\n",
    "    using_columns_statement = using_column_data_frame_1 == using_column_data_frame_2\n",
    "    joint_data_frame = data_frame_1.join(data_frame_2, using_columns_statement)\n",
    "\n",
    "    # To remove duplicated columns\n",
    "    joint_data_frame = joint_data_frame.drop(using_column_data_frame_1)\n",
    "\n",
    "    return joint_data_frame\n",
    "\n",
    "def write_spark_df_to_db(spark_df, table_name):\n",
    "    \"\"\"\n",
    "    This function writes Spark dataframe to DB\n",
    "    \"\"\"\n",
    "    spark_df \\\n",
    "        .write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .mode('overwrite') \\\n",
    "        .option(\"url\", POSTGRESQL_URL) \\\n",
    "        .option(\"user\", POSTGRESQL_USER) \\\n",
    "        .option(\"password\", POSTGRESQL_PASSWORD) \\\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .save()\n",
    "\n",
    "def read_dataset_from_db(spark_session, table_name):\n",
    "    \"\"\"\n",
    "    This function reads the clean dataset from the database\n",
    "    \"\"\"\n",
    "    spark_df = spark_session \\\n",
    "               .read \\\n",
    "               .format(\"jdbc\") \\\n",
    "               .option(\"url\", POSTGRESQL_URL) \\\n",
    "               .option(\"user\", POSTGRESQL_USER) \\\n",
    "               .option(\"password\", POSTGRESQL_PASSWORD) \\\n",
    "               .option(\"dbtable\", table_name) \\\n",
    "               .load()\n",
    "    spark_df.show()\n",
    "    return spark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4-) Datos de entrada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_entry_data_description():\n",
    "    \"\"\"\n",
    "    This function shows a description of all the entry data columns\n",
    "    \"\"\"\n",
    "    \n",
    "    # OIJ Dataset Explanation\n",
    "    print(\"\\na-) The first dataset contains information taken from the OIJ Police Statistics of Costa Rica.\")\n",
    "    \n",
    "    print(\"\\nb-) Columns description for OIJ dataset: \\n\")\n",
    "    print(\"Delito: Descripción del Delito\")\n",
    "    print(\"SubDelito: Descripción del SubDelito\")\n",
    "    print(\"Fecha: Fecha del Hecho\")\n",
    "    print(\"Hora: Rango de 3 horas del Hecho\")\n",
    "    print(\"Victima: Descripción de la Víctima \")\n",
    "    print(\"SubVictima: Descripción de la SubVíctima\")\n",
    "    print(\"Edad: Grupo de Edad que pertenece la Víctima\")\n",
    "    print(\"Genero: Género de la Víctima\")\n",
    "    print(\"Nacionalidad: Nacionalidad de la Víctima\")\n",
    "    print(\"Provincia: Provincia del Lugar del Hecho\")\n",
    "    print(\"Canton: Cantón del Lugar del Hecho\")\n",
    "    print(\"Distrito: Distrito del Lugar del Hecho\")\n",
    "    \n",
    "    # INEC Dataset Explanation\n",
    "    print(\"\\nc-) The second dataset contains information about Economic Indicators according to province, canton\")\n",
    "    print(\"    and district taken from INEC.\")\n",
    "    \n",
    "    print(\"\\nb-) Columns description for INEC dataset: \\n\")\n",
    "    print(\"Columna 1: Provincia, Cantón y Distrito\")\n",
    "    print(\"Columna 2: Población de 15 años y más\")\n",
    "    print(\"Columna 3: Tasa neta de participación\")\n",
    "    print(\"Columna 4: Tasa de ocupación\")\n",
    "    print(\"Columna 5: Tasa de desempleo abierto\")\n",
    "    print(\"Columna 6: Porcentaje de poblacion economicamente inactiva\")\n",
    "    print(\"Columna 7: Relación de depedencia económica\")\n",
    "    \n",
    "    # Show which columns is going to be predicted\n",
    "    print(\"\\ne-) The column that is going to be predicted is the type of Delito in San Jose province according to the canton.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5-) Cargado y preprocesamiento de datos antes de cruzarlos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "OIJ_DATAFRAME_UNNECESSARY_COLUMNS = [\"Fecha\", \"Hora\", \"SubVictima\", \"Provincia\"]\n",
    "INEC_DATAFRAME_UNNECESSARY_COLUMNS = [\"Porcentaje de poblacion economicamente inactiva\",\n",
    "                                      \"Relacion de dependencia economica\",\n",
    "                                      \"Porcentaje de poblacion ocupada - Sector Primario\",\n",
    "                                      \"Porcentaje de poblacion ocupada - Sector Secundario\",\n",
    "                                      \"Porcentaje de poblacion ocupada - Sector Terciario\"]\n",
    "\n",
    "def convert_categorical_values_to_numerical_from_df(spark_df, categorical_columns_list):\n",
    "    \"\"\"\n",
    "    This function creates a Spark DataFrame with all values as numerical\n",
    "    spark_df: Spark DataFrame\n",
    "    return the Spark DataFrame with all values as numerical\n",
    "    \"\"\"\n",
    "    # Convert categorical columns to numerical values\n",
    "    for categorical_column in categorical_columns_list:\n",
    "        new_column_name = \"{column_name}_index\".format(column_name = categorical_column)\n",
    "        indexer = StringIndexer(inputCol=categorical_column, outputCol=new_column_name)\n",
    "        spark_df = indexer.fit(spark_df).transform(spark_df)\n",
    "    \n",
    "    # Remove categorical columns\n",
    "    columns_to_drop = categorical_columns_list\n",
    "    spark_df = spark_df.drop(*columns_to_drop)\n",
    "    \n",
    "    # Rename new numerical columns\n",
    "    for categorical_column in categorical_columns_list:\n",
    "        new_column_name = \"{column_name}_index\".format(column_name = categorical_column)\n",
    "        spark_df = spark_df.withColumnRenamed(new_column_name, categorical_column)\n",
    "    \n",
    "    # Show converted data\n",
    "    print(\"- Show that the data has been converted successfully from categorical to numerical: \\n\")\n",
    "    spark_df.select(categorical_columns_list).show()\n",
    "\n",
    "    return spark_df\n",
    "\n",
    "def rename_oij_spark_dataframe_columns(spark_df):\n",
    "    \"\"\"\n",
    "    This function is necessary as OIJ Dataset is outdated since the CSV\n",
    "    they provide does not bring the time field\n",
    "    spark_df: Spark DataFrame\n",
    "    return the Spark DataFrame with right columns names\n",
    "    \"\"\"\n",
    "    spark_df = spark_df.withColumnRenamed('Victima', 'Hora')\n",
    "    spark_df = spark_df.withColumnRenamed('SubVictima', 'Victima')\n",
    "    spark_df = spark_df.withColumnRenamed('Edad', 'SubVictima')\n",
    "    spark_df = spark_df.withColumnRenamed('Genero', 'Edad')\n",
    "    spark_df = spark_df.withColumnRenamed('Nacionalidad', 'Genero')\n",
    "    spark_df = spark_df.withColumnRenamed('Provincia', 'Nacionalidad')\n",
    "    spark_df = spark_df.withColumnRenamed('Canton', 'Provincia')\n",
    "    spark_df = spark_df.withColumnRenamed('Distrito', 'Canton')\n",
    "    return spark_df\n",
    "\n",
    "def remove_spark_dataframe_trailing_whitespaces(spark_df):\n",
    "    \"\"\"\n",
    "    This function removes all trailing whitespaces in Spark DataFrame Columns\n",
    "    spark_df: Spark DataFrame\n",
    "    return the Spark DataFrame\n",
    "    \"\"\"\n",
    "    for column in spark_df.columns:\n",
    "        spark_df = spark_df.withColumn(column, trim(col(column)).cast(spark_df.schema[column].dataType))\n",
    "    return spark_df\n",
    "\n",
    "def convert_spark_dataframe_to_lower_case(spark_df):\n",
    "    \"\"\"\n",
    "    This function converts to lower case Spark DataFrame Columns\n",
    "    spark_df: Spark DataFrame\n",
    "    columns_list: Columns List\n",
    "    return the Spark DataFrame\n",
    "    \"\"\"\n",
    "    for column in spark_df.columns:\n",
    "        spark_df =  spark_df.withColumn(column, lower(col(column)).cast(spark_df.schema[column].dataType))\n",
    "    return spark_df\n",
    "    \n",
    "def create_and_clean_spark_dataframe_from_csv(spark_session,\n",
    "                                              csv_file_name,\n",
    "                                              schema_types_list):\n",
    "    \"\"\"\n",
    "    This function creates a Spark DataFrame from Dataset CSV file\n",
    "    spark_session: Spark Session\n",
    "    return the Spark DataFrame\n",
    "    \"\"\"\n",
    "    print(\"\\n- Loading and cleaning CSV input file: {name}\".format(name = csv_file_name))\n",
    "    \n",
    "    spark_df = spark_session \\\n",
    "      .read \\\n",
    "      .format(\"csv\") \\\n",
    "      .option(\"path\", csv_file_name) \\\n",
    "      .option(\"header\", True) \\\n",
    "      .schema(StructType(schema_types_list)) \\\n",
    "      .load()\n",
    "    \n",
    "    # Rename OIJ Spark DataFrame Columns\n",
    "    spark_df = rename_oij_spark_dataframe_columns(spark_df) \\\n",
    "               if csv_file_name == \"datos_delitos_oij_2011.csv\" \\\n",
    "               else spark_df\n",
    "    \n",
    "    # Clean whitespaces\n",
    "    spark_df = remove_spark_dataframe_trailing_whitespaces(spark_df)\n",
    "    \n",
    "    # Convert to lower case\n",
    "    spark_df = convert_spark_dataframe_to_lower_case(spark_df)\n",
    "    \n",
    "    return spark_df\n",
    "\n",
    "def remove_unnecessary_columns(spark_df, columns_to_remove_list):\n",
    "    \"\"\"\n",
    "    This function removes unnecessary columns for DataFrame\n",
    "    \"\"\"\n",
    "    columns_to_drop = columns_to_remove_list\n",
    "    spark_df = spark_df.drop(*columns_to_drop)\n",
    "    return spark_df\n",
    "\n",
    "def preprocessing_oij_data(spark_session):\n",
    "    \"\"\"\n",
    "    This function completes data preprocessing before training model\n",
    "    for OIJ Dataset\n",
    "    \"\"\"\n",
    "    schema_types_list = [StructField(\"Delito\", StringType()),\n",
    "                         StructField(\"SubDelito\", StringType()),\n",
    "                         StructField(\"Fecha\", StringType()),\n",
    "                         StructField(\"Victima\", StringType()),\n",
    "                         StructField(\"SubVictima\", StringType()),\n",
    "                         StructField(\"Edad\", StringType()),\n",
    "                         StructField(\"Genero\", StringType()),\n",
    "                         StructField(\"Nacionalidad\", StringType()),\n",
    "                         StructField(\"Provincia\", StringType()),\n",
    "                         StructField(\"Canton\", StringType()),\n",
    "                         StructField(\"Distrito\", StringType())]\n",
    "    \n",
    "    oij_spark_df = create_and_clean_spark_dataframe_from_csv(spark_session,\n",
    "                                                             \"datos_delitos_oij_2011.csv\",\n",
    "                                                             schema_types_list)\n",
    "    \n",
    "    oij_spark_df = remove_unnecessary_columns(oij_spark_df, OIJ_DATAFRAME_UNNECESSARY_COLUMNS)\n",
    "    \n",
    "    return oij_spark_df\n",
    "\n",
    "def preprocessing_inec_data(spark_session):\n",
    "    \"\"\"\n",
    "    This function completes data preprocessing before training model\n",
    "    for INEC Dataset\n",
    "    \"\"\"\n",
    "    schema_types_list = [StructField(\"Canton\", StringType()),\n",
    "                         StructField(\"Poblacion de 15 anos y mas\", IntegerType()),\n",
    "                         StructField(\"Tasa neta de participacion\", FloatType()),\n",
    "                         StructField(\"Tasa de ocupacion\", FloatType()),\n",
    "                         StructField(\"Tasa de desempleo abierto\", FloatType()),\n",
    "                         StructField(\"Porcentaje de poblacion economicamente inactiva\", FloatType()),\n",
    "                         StructField(\"Relacion de dependencia economica\", FloatType()),\n",
    "                         StructField(\"Porcentaje de poblacion ocupada - Sector Primario\", FloatType()),\n",
    "                         StructField(\"Porcentaje de poblacion ocupada - Sector Secundario\", FloatType()),\n",
    "                         StructField(\"Porcentaje de poblacion ocupada - Sector Terciario\", FloatType())]\n",
    "    \n",
    "    inec_spark_df = create_and_clean_spark_dataframe_from_csv(spark_session,\n",
    "                                                             \"datos_socioeconomicos_inec_2011.csv\",\n",
    "                                                             schema_types_list)\n",
    "    \n",
    "    inec_spark_df = remove_unnecessary_columns(inec_spark_df, INEC_DATAFRAME_UNNECESSARY_COLUMNS)\n",
    "    \n",
    "    return inec_spark_df\n",
    "\n",
    "def show_preprocessing_spark_dataframe(spark_df, columns_list):\n",
    "    \"\"\"\n",
    "    This function completes data preprocessing before training model\n",
    "    \"\"\"\n",
    "    print(\"\\n1-) Definition of schema: \\n\")\n",
    "    spark_df.printSchema()\n",
    "    \n",
    "    print(\"2-) Show that the data has been loaded successfully by selecting a couple of rows: \\n\")\n",
    "    spark_df.select(columns_list).show()\n",
    "    \n",
    "\n",
    "def data_preprocessing(spark_session):\n",
    "    \"\"\"\n",
    "    This function completes data preprocessing for INEC and OIJ before training model\n",
    "    \"\"\"\n",
    "    # Loading and cleaning OIJ CSV input file data.\n",
    "    oij_spark_df = preprocessing_oij_data(spark_session)\n",
    "    \n",
    "    # Show Preprocessing OIJ DataFrame\n",
    "    show_preprocessing_spark_dataframe(oij_spark_df,\n",
    "                                       [\"Delito\", \"SubDelito\", \"Genero\", \"Canton\"])\n",
    "    \n",
    "    # Convert all OIJ Categorical Values to Numerical Values\n",
    "    oij_spark_df = convert_categorical_values_to_numerical_from_df(oij_spark_df,\n",
    "                                                                   [\"Delito\", \"SubDelito\",\n",
    "                                                                    \"Victima\", \"Edad\", \"Genero\",\n",
    "                                                                    \"Nacionalidad\"])\n",
    "    \n",
    "    # Loading and cleaning INEC CSV input file data.\n",
    "    inec_spark_df = preprocessing_inec_data(spark_session)\n",
    "    \n",
    "    # Show Preprocessing INEC DataFrame\n",
    "    show_preprocessing_spark_dataframe(inec_spark_df,\n",
    "                                       [\"Canton\",\n",
    "                                        \"Poblacion de 15 anos y mas\",\n",
    "                                        \"Tasa de desempleo abierto\"])\n",
    "    \n",
    "    return oij_spark_df, inec_spark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6-) Materialización en Postgresql**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_LIST = ['SubDelito', 'Victima', 'Edad',\n",
    "                 'Genero', 'Nacionalidad',\n",
    "                 'Poblacion de 15 anos y mas', 'Tasa neta de participacion',\n",
    "                 'Tasa de ocupacion', 'Tasa de desempleo abierto', 'Canton', 'Delito']\n",
    "\n",
    "def create_joint_spark_data_frames(oij_data_frame, inec_data_frame):\n",
    "    \"\"\"\n",
    "    This function creates the data frame of the joint of the two datasets\n",
    "    oij_data_frame: DataFrame with the students info\n",
    "    inec_data_frame: DataFrame with the courses info\n",
    "    return the joint Spark DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nThe data union between OIJ DataFrame and INEC DataFrame is the following: \\n\")\n",
    "    joint_oij_and_inec_df = join_spark_data_frames(oij_data_frame,\n",
    "                                                   inec_data_frame,\n",
    "                                                   oij_data_frame.Canton,\n",
    "                                                   inec_data_frame.Canton)\n",
    "    \n",
    "    joint_oij_and_inec_df.select([\"Delito\", \"Genero\", \"Canton\",\n",
    "                                  \"Poblacion de 15 anos y mas\",\n",
    "                                  \"Tasa de desempleo abierto\",\n",
    "                                  \"Tasa de ocupacion\"]).show(20)\n",
    "\n",
    "    return joint_oij_and_inec_df\n",
    "\n",
    "def transform_sparse_vector_df_to_dense_vector_df(spark_df):\n",
    "    \"\"\"\n",
    "    This function transforms a sparse vector to a dense vector df\n",
    "    \"\"\"\n",
    "    toDense = lambda v: Vectors.dense(v.toArray())\n",
    "    toDenseUdf = udf(toDense, VectorUDT())\n",
    "    spark_df = spark_df.withColumn('features', toDenseUdf('features'))\n",
    "    return spark_df\n",
    "\n",
    "def transform_spark_df_to_features_vector_df(spark_df, remove_delito=False):\n",
    "    \"\"\"\n",
    "    This function transforms a spark df to a features vector df\n",
    "    \"\"\"\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=FEATURES_LIST[:-1] if remove_delito else \\\n",
    "                  FEATURES_LIST,\n",
    "        outputCol='features')\n",
    "    \n",
    "    vector_df = assembler.transform(spark_df)\n",
    "    vector_df = vector_df.select(['features', 'Delito'])\n",
    "    \n",
    "    # Converte Sparse Vectors to Dense Vectors\n",
    "    vector_df = transform_sparse_vector_df_to_dense_vector_df(vector_df)\n",
    "\n",
    "    return vector_df\n",
    "\n",
    "def normalize_data(spark_df):\n",
    "    \"\"\"\n",
    "    This function normalize data before training the model\n",
    "    \"\"\"\n",
    "    vector_df = transform_spark_df_to_features_vector_df(spark_df, remove_delito=True)\n",
    "    standard_scaler = StandardScaler(inputCol='features', outputCol='scaled_features')\n",
    "    scale_model = standard_scaler.fit(vector_df)\n",
    "    scaled_df = scale_model.transform(vector_df)\n",
    "    \n",
    "    return scaled_df\n",
    "\n",
    "def convert_normalized_df_to_valid_table_for_db(normalized_df):\n",
    "    \"\"\"\n",
    "    This function converts normalize features vector into a valid DF for DB\n",
    "    \"\"\"\n",
    "    normalized_valid_db_format_df = normalized_df.select('scaled_features', 'Delito')\n",
    "    normalized_lambda_function = lambda x:[float(y) for y in x['scaled_features']]+[x['Delito']]\n",
    "    normalized_valid_db_format_df = normalized_valid_db_format_df.rdd.map(normalized_lambda_function).toDF(FEATURES_LIST)\n",
    "    \n",
    "    return normalized_valid_db_format_df\n",
    "\n",
    "def write_and_read_oij_and_inec_df(spark_session, oij_spark_df, inec_spark_df):\n",
    "    \"\"\"\n",
    "    This function write and read OIJ and INEC DF to DB\n",
    "    \"\"\"\n",
    "    # Writing OIJ and INEC Dataframes to DB first\n",
    "    print(\"\\n- Before joining the data, is important to write to the DB first. \")\n",
    "    write_spark_df_to_db(oij_spark_df, \"oij\")\n",
    "    write_spark_df_to_db(inec_spark_df, \"inec\")\n",
    "    \n",
    "    # Reading OIJ and INEC Dataframes from DB\n",
    "    print(\"\\n- After writing OIJ and INEC to the DB, now is necessary to read those clean datasets. \")\n",
    "    print(\"\\n- OIJ Dataset from DB is: \")\n",
    "    read_dataset_from_db(spark_session, \"oij\")\n",
    "    print(\"\\n- INEC Dataset from DB is: \")\n",
    "    read_dataset_from_db(spark_session, \"inec\")\n",
    "    \n",
    "    return oij_spark_df, inec_spark_df\n",
    "\n",
    "def materialization_to_postgresql(spark_session, oij_spark_df, inec_spark_df):\n",
    "    \"\"\"\n",
    "    This function materialize data to write to DB\n",
    "    \"\"\"\n",
    "    # Writing and Reading OIJ and INEC Dataframes\n",
    "    oij_spark_df, inec_spark_df = write_and_read_oij_and_inec_df(spark_session,\n",
    "                                                                 oij_spark_df, inec_spark_df)\n",
    "    \n",
    "    # Joint Spark Data Frames\n",
    "    joint_data_frame = create_joint_spark_data_frames(oij_spark_df,\n",
    "                                                      inec_spark_df)\n",
    "    \n",
    "    # Convert Canton Categorical Value to Numerical Values\n",
    "    joint_data_frame = convert_categorical_values_to_numerical_from_df(joint_data_frame,\n",
    "                                                                       [\"Canton\"])\n",
    "    \n",
    "    # Normalizing the data\n",
    "    print(\"\\n- After normalizing the data, the DataFrame looks like this: \\n\")\n",
    "    normalized_df = normalize_data(joint_data_frame)\n",
    "    normalized_df.show()\n",
    "    \n",
    "    # Convert to valid format for DB\n",
    "    print(\"\\ng-) Writing to DB: \")\n",
    "    print(\"\\ng.1-) Before writing the normalized Dataframe, is necessary to convert it to a valid table for DB: \")\n",
    "    normalized_valid_db_df = convert_normalized_df_to_valid_table_for_db(normalized_df)\n",
    "    print(\"- After convert it to a valid table for DB, the DataFrame looks like this: \\n\")\n",
    "    normalized_valid_db_df.select(FEATURES_LIST[:-5]).show()\n",
    "    \n",
    "    # Writing to DB\n",
    "    write_spark_df_to_db(normalized_valid_db_df, \"datos_cruzados\")\n",
    "    print(\"- In order to check that this table was written correctly into DB,\")\n",
    "    print(\"  you can go to PostgreSQL and check the table named datos_cruzados.\")\n",
    "    print(\"  It should look exactly the same.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7-) Modelos de predicción**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_data(spark_df):\n",
    "    \"\"\"\n",
    "    This function splits train and test data\n",
    "    \"\"\"\n",
    "    train, test = spark_df.randomSplit([0.8, 0.2], seed=12345)\n",
    "    return train, test\n",
    "\n",
    "def select_data_for_training_models(spark_session):\n",
    "    \"\"\"\n",
    "    This function selects the data for training two models\n",
    "    \"\"\"\n",
    "    print(\"\\n- Read dataset from DB:\")\n",
    "    clean_spark_df = read_dataset_from_db(spark_session, \"datos_cruzados\")\n",
    "    \n",
    "    print(\"\\n- Convert features into one single vector:\")\n",
    "    clean_vector_features_df = transform_spark_df_to_features_vector_df(clean_spark_df, remove_delito=True)\n",
    "    clean_vector_features_df.show()\n",
    "    \n",
    "    # Split train and test data\n",
    "    train_data, test_data = split_train_test_data(clean_vector_features_df)\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "def set_random_forest_classifier_parameter_grid(random_forest):\n",
    "    \"\"\"\n",
    "    This function set parameter grid for Cross Validation\n",
    "    \"\"\"\n",
    "    param_grid = ParamGridBuilder()\\\n",
    "    .addGrid(random_forest.maxDepth, [2, 4])\\\n",
    "    .addGrid(random_forest.maxBins, [20])\\\n",
    "    .addGrid(random_forest.numTrees, [5])\\\n",
    "    .build()\n",
    "    \n",
    "    return param_grid\n",
    "\n",
    "def building_model_using_decision_tree_classifier(train_data, test_data):\n",
    "    \"\"\"\n",
    "    This function builds a classification model using\n",
    "    Decision Tree Classifier\n",
    "    \"\"\"\n",
    "    decision_tree_classifier = DecisionTreeClassifier(labelCol=\"Delito\",\n",
    "                                                      featuresCol=\"features\")\n",
    "    \n",
    "    model = decision_tree_classifier.fit(train_data)\n",
    "    predict_train_data = model.transform(train_data)\n",
    "    predict_test_data = model.transform(test_data)\n",
    "    \n",
    "    return predict_train_data, predict_test_data\n",
    "\n",
    "def building_model_using_random_forest(train_data, test_data):\n",
    "    \"\"\"\n",
    "    This function builds a classification model using\n",
    "    Random Forest\n",
    "    \"\"\"\n",
    "    random_forest = RandomForestClassifier(labelCol=\"Delito\",\n",
    "                                           featuresCol=\"features\")\n",
    "    \n",
    "    # Set parameter grid\n",
    "    parameter_grid = set_random_forest_classifier_parameter_grid(random_forest)\n",
    "    \n",
    "    # Set evaluator\n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\",\n",
    "                                                  labelCol=\"Delito\")\n",
    "    \n",
    "    # Set Cross Validation\n",
    "    cross_validation = CrossValidator(estimator=random_forest,\n",
    "                                      estimatorParamMaps=parameter_grid,\n",
    "                                      evaluator=evaluator, numFolds=3)\n",
    "    \n",
    "    # Run Cross Validations\n",
    "    cross_validation_model = cross_validation.fit(train_data)\n",
    "\n",
    "    predict_train_data = cross_validation_model.transform(train_data)\n",
    "    predict_test_data = cross_validation_model.transform(test_data)\n",
    "    \n",
    "    return predict_train_data, predict_test_data\n",
    "\n",
    "def training_model_1(train_data, test_data):\n",
    "    \"\"\"\n",
    "    This function trains a decission tree model\n",
    "    \"\"\"\n",
    "    print(\"\\n- Building a model using Decision tree classifier.\")\n",
    "    predict_train_data, predict_test_data = building_model_using_decision_tree_classifier(train_data, test_data)\n",
    "    return predict_train_data, predict_test_data\n",
    "\n",
    "def training_model_2(train_data, test_data):\n",
    "    \"\"\"\n",
    "    This function trains a random forest model\n",
    "    \"\"\"\n",
    "    print(\"\\n- Building a model using Random Forest Classifier. \\n\")\n",
    "    predict_train_data, predict_test_data = building_model_using_random_forest(train_data, test_data)\n",
    "    return predict_train_data, predict_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **8-) Análisis de resultados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluating_decision_tree_model(predict_train_data, predict_test_data):\n",
    "    \"\"\"\n",
    "    This function evaluates the random forest model\n",
    "    \"\"\"\n",
    "    print(\"\\nMake predictions and evaluates the Decision Tree Classifier Model: \\n\")\n",
    "    \n",
    "    print(\"\\n1. Make predictions with Decision Tree Classifier Model: \")\n",
    "    predict_test_data.select(\"Delito\", \"prediction\").show(20)\n",
    "    \n",
    "    print(\"\\n2. Evaluates Decision Tree Classifier Model: \\n\")\n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"Delito\")\n",
    "    predict_test_data.select(\"Delito\", \"prediction\", \"probability\").show(15)\n",
    "    print(\"The area under ROC for train data is {}\".format(evaluator.evaluate(predict_train_data)))\n",
    "    print(\"The area under ROC for test data is {}\".format(evaluator.evaluate(predict_test_data)))\n",
    "    \n",
    "def evaluating_random_forest_model(predict_train_data, predict_test_data):\n",
    "    \"\"\"\n",
    "    This function evaluates the random forest model\n",
    "    \"\"\"\n",
    "    print(\"\\nMake predictions and evaluates the Random Forest Classifier Model: \\n\")\n",
    "    \n",
    "    print(\"\\n1. Make predictions with Random Forest Classifier Model: \")\n",
    "    predict_test_data.select(\"Delito\", \"prediction\").show(20)\n",
    "    \n",
    "    print(\"\\n2. Evaluates Random Forest Classifier Model: \\n\")\n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"Delito\")\n",
    "    predict_test_data.select(\"Delito\", \"prediction\", \"probability\").show(15)\n",
    "    print(\"The area under ROC for train data is {}\".format(evaluator.evaluate(predict_train_data)))\n",
    "    print(\"The area under ROC for test data is {}\".format(evaluator.evaluate(predict_test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Qz_1dBJJgFd"
   },
   "source": [
    "# **9-) Función main() para ejecutar el programa principal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "JSp6jIOgBXzs",
    "outputId": "ec260664-da48-425d-ad14-307d7e483b9d",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Entry Data Description\n",
      "\n",
      "a-) The first dataset contains information taken from the OIJ Police Statistics of Costa Rica.\n",
      "\n",
      "b-) Columns description for OIJ dataset: \n",
      "\n",
      "Delito: Descripción del Delito\n",
      "SubDelito: Descripción del SubDelito\n",
      "Fecha: Fecha del Hecho\n",
      "Hora: Rango de 3 horas del Hecho\n",
      "Victima: Descripción de la Víctima \n",
      "SubVictima: Descripción de la SubVíctima\n",
      "Edad: Grupo de Edad que pertenece la Víctima\n",
      "Genero: Género de la Víctima\n",
      "Nacionalidad: Nacionalidad de la Víctima\n",
      "Provincia: Provincia del Lugar del Hecho\n",
      "Canton: Cantón del Lugar del Hecho\n",
      "Distrito: Distrito del Lugar del Hecho\n",
      "\n",
      "c-) The second dataset contains information about Economic Indicators according to province, canton\n",
      "    and district taken from INEC.\n",
      "\n",
      "b-) Columns description for INEC dataset: \n",
      "\n",
      "Columna 1: Provincia, Cantón y Distrito\n",
      "Columna 2: Población de 15 años y más\n",
      "Columna 3: Tasa neta de participación\n",
      "Columna 4: Tasa de ocupación\n",
      "Columna 5: Tasa de desempleo abierto\n",
      "Columna 6: Porcentaje de poblacion economicamente inactiva\n",
      "Columna 7: Relación de depedencia económica\n",
      "\n",
      "e-) The column that is going to be predicted is the type of Delito in San Jose province according to the canton.\n",
      "\n",
      "Step 2: Data Preprocessing\n",
      "\n",
      "- Loading and cleaning CSV input file: datos_delitos_oij_2011.csv\n",
      "\n",
      "1-) Definition of schema: \n",
      "\n",
      "root\n",
      " |-- Delito: string (nullable = true)\n",
      " |-- SubDelito: string (nullable = true)\n",
      " |-- Victima: string (nullable = true)\n",
      " |-- Edad: string (nullable = true)\n",
      " |-- Genero: string (nullable = true)\n",
      " |-- Nacionalidad: string (nullable = true)\n",
      " |-- Canton: string (nullable = true)\n",
      "\n",
      "2-) Show that the data has been loaded successfully by selecting a couple of rows: \n",
      "\n",
      "+------+-----------+------+-------------------+\n",
      "|Delito|  SubDelito|Genero|             Canton|\n",
      "+------+-----------+------+-------------------+\n",
      "|asalto|arma blanca|hombre|      perez zeledon|\n",
      "|asalto|arma blanca| mujer|           san jose|\n",
      "|asalto|arma blanca|hombre|           san jose|\n",
      "|asalto|arma blanca|hombre|           san jose|\n",
      "|asalto|arma blanca|hombre|         curridabat|\n",
      "|asalto|arma blanca|hombre|           san jose|\n",
      "|asalto|arma blanca| mujer|      montes de oca|\n",
      "|asalto|arma blanca|hombre|      perez zeledon|\n",
      "|asalto|arma blanca|hombre|vasquez de coronado|\n",
      "|asalto|arma blanca|hombre|           san jose|\n",
      "|asalto|arma blanca|hombre|      perez zeledon|\n",
      "|asalto|arma blanca|hombre|            moravia|\n",
      "|asalto|arma blanca| mujer|      montes de oca|\n",
      "|asalto|arma blanca| mujer|       desamparados|\n",
      "|asalto|arma blanca|hombre|       desamparados|\n",
      "|asalto|arma blanca| mujer|      montes de oca|\n",
      "|asalto|arma blanca|hombre|       desamparados|\n",
      "|asalto|arma blanca|hombre|           san jose|\n",
      "|asalto|arma blanca|hombre|         goicoechea|\n",
      "|asalto|arma blanca|hombre|         curridabat|\n",
      "+------+-----------+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "- Show that the data has been converted successfully from categorical to numerical: \n",
      "\n",
      "+------+---------+-------+----+------+------------+\n",
      "|Delito|SubDelito|Victima|Edad|Genero|Nacionalidad|\n",
      "+------+---------+-------+----+------+------------+\n",
      "|   0.0|      1.0|    0.0| 1.0|   0.0|         0.0|\n",
      "|   0.0|      1.0|    0.0| 1.0|   1.0|         1.0|\n",
      "|   0.0|      1.0|    0.0| 0.0|   0.0|         0.0|\n",
      "|   0.0|      1.0|    0.0| 0.0|   0.0|        24.0|\n",
      "|   0.0|      1.0|    0.0| 0.0|   0.0|         0.0|\n",
      "|   0.0|      1.0|    0.0| 0.0|   0.0|         0.0|\n",
      "|   0.0|      1.0|    0.0| 0.0|   1.0|         9.0|\n",
      "|   0.0|      1.0|    0.0| 0.0|   0.0|         0.0|\n",
      "|   0.0|      1.0|    0.0| 0.0|   0.0|         0.0|\n",
      "|   0.0|      1.0|    0.0| 0.0|   0.0|         0.0|\n",
      "|   0.0|      1.0|    0.0| 0.0|   0.0|         0.0|\n",
      "|   0.0|      1.0|    0.0| 1.0|   0.0|         0.0|\n",
      "|   0.0|      1.0|    0.0| 0.0|   1.0|         0.0|\n",
      "|   0.0|      1.0|    0.0| 0.0|   1.0|         0.0|\n",
      "|   0.0|      1.0|    0.0| 2.0|   0.0|         0.0|\n",
      "|   0.0|      1.0|    0.0| 0.0|   1.0|         0.0|\n",
      "|   0.0|      1.0|    0.0| 0.0|   0.0|         1.0|\n",
      "|   0.0|      1.0|    0.0| 0.0|   0.0|         0.0|\n",
      "|   0.0|      1.0|    0.0| 0.0|   0.0|         0.0|\n",
      "|   0.0|      1.0|    0.0| 1.0|   0.0|         0.0|\n",
      "+------+---------+-------+----+------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "- Loading and cleaning CSV input file: datos_socioeconomicos_inec_2011.csv\n",
      "\n",
      "1-) Definition of schema: \n",
      "\n",
      "root\n",
      " |-- Canton: string (nullable = true)\n",
      " |-- Poblacion de 15 anos y mas: integer (nullable = true)\n",
      " |-- Tasa neta de participacion: float (nullable = true)\n",
      " |-- Tasa de ocupacion: float (nullable = true)\n",
      " |-- Tasa de desempleo abierto: float (nullable = true)\n",
      "\n",
      "2-) Show that the data has been loaded successfully by selecting a couple of rows: \n",
      "\n",
      "+-------------------+--------------------------+-------------------------+\n",
      "|             Canton|Poblacion de 15 anos y mas|Tasa de desempleo abierto|\n",
      "+-------------------+--------------------------+-------------------------+\n",
      "|           san jose|                    225856|                      3.9|\n",
      "|             escazu|                     44797|                      3.0|\n",
      "|       desamparados|                    159292|                      4.0|\n",
      "|           puriscal|                     25774|                      3.1|\n",
      "|            tarrazu|                     11800|                      2.8|\n",
      "|             aserri|                     43396|                      3.2|\n",
      "|               mora|                     20414|                      3.2|\n",
      "|         goicoechea|                     90537|                      3.7|\n",
      "|          santa ana|                     38096|                      2.3|\n",
      "|         alajuelita|                     56704|                      3.8|\n",
      "|vazquez de coronado|                     47697|                      3.0|\n",
      "|             acosta|                     15270|                      2.1|\n",
      "|              tibas|                     52194|                      3.5|\n",
      "|            moravia|                     45444|                      3.2|\n",
      "|      montes de oca|                     41561|                      2.9|\n",
      "|         turrubares|                      4133|                      3.9|\n",
      "|               dota|                      5164|                      1.8|\n",
      "|         curridabat|                     51916|                      3.0|\n",
      "|      perez zeledon|                     98186|                      3.4|\n",
      "| leon cortes castro|                      9084|                      2.4|\n",
      "+-------------------+--------------------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Step 3: Materialization to PostgreSQL\n",
      "\n",
      "- Before joining the data, is important to write to the DB first. \n",
      "\n",
      "- After writing OIJ and INEC to the DB, now is necessary to read those clean datasets. \n",
      "\n",
      "- OIJ Dataset from DB is: \n",
      "+-------------------+------+---------+-------+----+------+------------+\n",
      "|             Canton|Delito|SubDelito|Victima|Edad|Genero|Nacionalidad|\n",
      "+-------------------+------+---------+-------+----+------+------------+\n",
      "|      perez zeledon|   0.0|      1.0|    0.0| 1.0|   0.0|         0.0|\n",
      "|           san jose|   0.0|      1.0|    0.0| 1.0|   1.0|         1.0|\n",
      "|           san jose|   0.0|      1.0|    0.0| 0.0|   0.0|         0.0|\n",
      "|           san jose|   0.0|      1.0|    0.0| 0.0|   0.0|        24.0|\n",
      "|         curridabat|   0.0|      1.0|    0.0| 0.0|   0.0|         0.0|\n",
      "|           san jose|   0.0|      1.0|    0.0| 0.0|   0.0|         0.0|\n",
      "|      montes de oca|   0.0|      1.0|    0.0| 0.0|   1.0|         9.0|\n",
      "|      perez zeledon|   0.0|      1.0|    0.0| 0.0|   0.0|         0.0|\n",
      "|vasquez de coronado|   0.0|      1.0|    0.0| 0.0|   0.0|         0.0|\n",
      "|           san jose|   0.0|      1.0|    0.0| 0.0|   0.0|         0.0|\n",
      "|      perez zeledon|   0.0|      1.0|    0.0| 0.0|   0.0|         0.0|\n",
      "|            moravia|   0.0|      1.0|    0.0| 1.0|   0.0|         0.0|\n",
      "|      montes de oca|   0.0|      1.0|    0.0| 0.0|   1.0|         0.0|\n",
      "|       desamparados|   0.0|      1.0|    0.0| 0.0|   1.0|         0.0|\n",
      "|       desamparados|   0.0|      1.0|    0.0| 2.0|   0.0|         0.0|\n",
      "|      montes de oca|   0.0|      1.0|    0.0| 0.0|   1.0|         0.0|\n",
      "|       desamparados|   0.0|      1.0|    0.0| 0.0|   0.0|         1.0|\n",
      "|           san jose|   0.0|      1.0|    0.0| 0.0|   0.0|         0.0|\n",
      "|         goicoechea|   0.0|      1.0|    0.0| 0.0|   0.0|         0.0|\n",
      "|         curridabat|   0.0|      1.0|    0.0| 1.0|   0.0|         0.0|\n",
      "+-------------------+------+---------+-------+----+------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "- INEC Dataset from DB is: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------------+--------------------------+-----------------+-------------------------+\n",
      "|             Canton|Poblacion de 15 anos y mas|Tasa neta de participacion|Tasa de ocupacion|Tasa de desempleo abierto|\n",
      "+-------------------+--------------------------+--------------------------+-----------------+-------------------------+\n",
      "|           san jose|                    225856|                      56.7|             54.5|                      3.9|\n",
      "|             escazu|                     44797|                      60.7|             58.9|                      3.0|\n",
      "|       desamparados|                    159292|                      57.0|             54.7|                      4.0|\n",
      "|           puriscal|                     25774|                      50.7|             49.1|                      3.1|\n",
      "|            tarrazu|                     11800|                      51.2|             49.7|                      2.8|\n",
      "|             aserri|                     43396|                      54.5|             52.7|                      3.2|\n",
      "|               mora|                     20414|                      55.2|             53.4|                      3.2|\n",
      "|         goicoechea|                     90537|                      56.8|             54.7|                      3.7|\n",
      "|          santa ana|                     38096|                      61.8|             60.4|                      2.3|\n",
      "|         alajuelita|                     56704|                      56.0|             53.9|                      3.8|\n",
      "|vazquez de coronado|                     47697|                      58.4|             56.6|                      3.0|\n",
      "|             acosta|                     15270|                      49.6|             48.5|                      2.1|\n",
      "|              tibas|                     52194|                      56.0|             54.1|                      3.5|\n",
      "|            moravia|                     45444|                      58.7|             56.8|                      3.2|\n",
      "|      montes de oca|                     41561|                      58.8|             57.1|                      2.9|\n",
      "|         turrubares|                      4133|                      48.2|             46.3|                      3.9|\n",
      "|               dota|                      5164|                      51.2|             50.3|                      1.8|\n",
      "|         curridabat|                     51916|                      58.9|             57.1|                      3.0|\n",
      "|      perez zeledon|                     98186|                      48.0|             46.4|                      3.4|\n",
      "| leon cortes castro|                      9084|                      47.2|             46.0|                      2.4|\n",
      "+-------------------+--------------------------+--------------------------+-----------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "The data union between OIJ DataFrame and INEC DataFrame is the following: \n",
      "\n",
      "+------+------+-------------+--------------------------+-------------------------+-----------------+\n",
      "|Delito|Genero|       Canton|Poblacion de 15 anos y mas|Tasa de desempleo abierto|Tasa de ocupacion|\n",
      "+------+------+-------------+--------------------------+-------------------------+-----------------+\n",
      "|   0.0|   0.0|perez zeledon|                     98186|                      3.4|             46.4|\n",
      "|   0.0|   1.0|     san jose|                    225856|                      3.9|             54.5|\n",
      "|   0.0|   0.0|     san jose|                    225856|                      3.9|             54.5|\n",
      "|   0.0|   0.0|     san jose|                    225856|                      3.9|             54.5|\n",
      "|   0.0|   0.0|   curridabat|                     51916|                      3.0|             57.1|\n",
      "|   0.0|   0.0|     san jose|                    225856|                      3.9|             54.5|\n",
      "|   0.0|   1.0|montes de oca|                     41561|                      2.9|             57.1|\n",
      "|   0.0|   0.0|perez zeledon|                     98186|                      3.4|             46.4|\n",
      "|   0.0|   0.0|     san jose|                    225856|                      3.9|             54.5|\n",
      "|   0.0|   0.0|perez zeledon|                     98186|                      3.4|             46.4|\n",
      "|   0.0|   0.0|      moravia|                     45444|                      3.2|             56.8|\n",
      "|   0.0|   1.0|montes de oca|                     41561|                      2.9|             57.1|\n",
      "|   0.0|   1.0| desamparados|                    159292|                      4.0|             54.7|\n",
      "|   0.0|   0.0| desamparados|                    159292|                      4.0|             54.7|\n",
      "|   0.0|   1.0|montes de oca|                     41561|                      2.9|             57.1|\n",
      "|   0.0|   0.0| desamparados|                    159292|                      4.0|             54.7|\n",
      "|   0.0|   0.0|     san jose|                    225856|                      3.9|             54.5|\n",
      "|   0.0|   0.0|   goicoechea|                     90537|                      3.7|             54.7|\n",
      "|   0.0|   0.0|   curridabat|                     51916|                      3.0|             57.1|\n",
      "|   0.0|   1.0| desamparados|                    159292|                      4.0|             54.7|\n",
      "+------+------+-------------+--------------------------+-------------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "- Show that the data has been converted successfully from categorical to numerical: \n",
      "\n",
      "+------+\n",
      "|Canton|\n",
      "+------+\n",
      "|   3.0|\n",
      "|   0.0|\n",
      "|   0.0|\n",
      "|   0.0|\n",
      "|   5.0|\n",
      "|   0.0|\n",
      "|   2.0|\n",
      "|   3.0|\n",
      "|   0.0|\n",
      "|   3.0|\n",
      "|   9.0|\n",
      "|   2.0|\n",
      "|   1.0|\n",
      "|   1.0|\n",
      "|   2.0|\n",
      "|   1.0|\n",
      "|   0.0|\n",
      "|   4.0|\n",
      "|   5.0|\n",
      "|   1.0|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "- After normalizing the data, the DataFrame looks like this: \n",
      "\n",
      "+--------------------+------+--------------------+\n",
      "|            features|Delito|     scaled_features|\n",
      "+--------------------+------+--------------------+\n",
      "|[1.0,0.0,1.0,0.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,1.0,1.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,0.0,0.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,0.0,0.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,0.0,0.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,0.0,0.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,0.0,1.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,0.0,0.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,0.0,0.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,0.0,0.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,1.0,0.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,0.0,1.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,0.0,1.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,2.0,0.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,0.0,1.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,0.0,0.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,0.0,0.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,0.0,0.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,1.0,0.0,...|   0.0|[0.31499290187266...|\n",
      "|[1.0,0.0,0.0,1.0,...|   0.0|[0.31499290187266...|\n",
      "+--------------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "g-) Writing to DB: \n",
      "\n",
      "g.1-) Before writing the normalized Dataframe, is necessary to convert it to a valid table for DB: \n",
      "- After convert it to a valid table for DB, the DataFrame looks like this: \n",
      "\n",
      "+------------------+-------+------------------+-----------------+-------------------+--------------------------+\n",
      "|         SubDelito|Victima|              Edad|           Genero|       Nacionalidad|Poblacion de 15 anos y mas|\n",
      "+------------------+-------+------------------+-----------------+-------------------+--------------------------+\n",
      "|0.3149929018726635|    0.0|2.4305252824998718|              0.0|                0.0|          1.25541377491375|\n",
      "|0.3149929018726635|    0.0|2.4305252824998718|2.083182920207937|0.20442058775065666|         2.887812249678365|\n",
      "|0.3149929018726635|    0.0|               0.0|              0.0|                0.0|         2.887812249678365|\n",
      "|0.3149929018726635|    0.0|               0.0|              0.0|   4.90609410601576|         2.887812249678365|\n",
      "|0.3149929018726635|    0.0|               0.0|              0.0|                0.0|        0.6638019833624167|\n",
      "|0.3149929018726635|    0.0|               0.0|              0.0|                0.0|         2.887812249678365|\n",
      "|0.3149929018726635|    0.0|               0.0|2.083182920207937|   1.83978528975591|        0.5314021540666731|\n",
      "|0.3149929018726635|    0.0|               0.0|              0.0|                0.0|          1.25541377491375|\n",
      "|0.3149929018726635|    0.0|               0.0|              0.0|                0.0|         2.887812249678365|\n",
      "|0.3149929018726635|    0.0|               0.0|              0.0|                0.0|          1.25541377491375|\n",
      "|0.3149929018726635|    0.0|2.4305252824998718|              0.0|                0.0|        0.5810504917929282|\n",
      "|0.3149929018726635|    0.0|               0.0|2.083182920207937|                0.0|        0.5314021540666731|\n",
      "|0.3149929018726635|    0.0|               0.0|2.083182920207937|                0.0|         2.036719807646315|\n",
      "|0.3149929018726635|    0.0|4.8610505649997435|              0.0|                0.0|         2.036719807646315|\n",
      "|0.3149929018726635|    0.0|               0.0|2.083182920207937|                0.0|        0.5314021540666731|\n",
      "|0.3149929018726635|    0.0|               0.0|              0.0|0.20442058775065666|         2.036719807646315|\n",
      "|0.3149929018726635|    0.0|               0.0|              0.0|                0.0|         2.887812249678365|\n",
      "|0.3149929018726635|    0.0|               0.0|              0.0|                0.0|         1.157613070492394|\n",
      "|0.3149929018726635|    0.0|2.4305252824998718|              0.0|                0.0|        0.6638019833624167|\n",
      "|0.3149929018726635|    0.0|               0.0|2.083182920207937|                0.0|         2.036719807646315|\n",
      "+------------------+-------+------------------+-----------------+-------------------+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- In order to check that this table was written correctly into DB,\n",
      "  you can go to PostgreSQL and check the table named datos_cruzados.\n",
      "  It should look exactly the same.\n",
      "\n",
      "Step 4: Training Models\n",
      "\n",
      "- Read dataset from DB:\n",
      "+------------------+-------+------------------+-----------------+-------------------+--------------------------+--------------------------+------------------+-------------------------+------------------+------+\n",
      "|         SubDelito|Victima|              Edad|           Genero|       Nacionalidad|Poblacion de 15 anos y mas|Tasa neta de participacion| Tasa de ocupacion|Tasa de desempleo abierto|            Canton|Delito|\n",
      "+------------------+-------+------------------+-----------------+-------------------+--------------------------+--------------------------+------------------+-------------------------+------------------+------+\n",
      "|0.3149929018726635|    0.0|2.4305252824998718|              0.0|                0.0|          1.25541377491375|         19.38275964575944| 19.11629268809971|        8.703035114014002|0.9842927375078734|   0.0|\n",
      "|0.3149929018726635|    0.0|2.4305252824998718|2.083182920207937|0.20442058775065666|         2.887812249678365|        22.895885139634007|22.453403388798698|        9.982893183117008|               0.0|   0.0|\n",
      "|0.3149929018726635|    0.0|               0.0|              0.0|                0.0|         2.887812249678365|        22.895885139634007|22.453403388798698|        9.982893183117008|               0.0|   0.0|\n",
      "|0.3149929018726635|    0.0|               0.0|              0.0|   4.90609410601576|         2.887812249678365|        22.895885139634007|22.453403388798698|        9.982893183117008|               0.0|   0.0|\n",
      "|0.3149929018726635|    0.0|               0.0|              0.0|                0.0|        0.6638019833624167|        23.784261931478646| 23.52457429796754|        7.679148414618043|1.6404878958464555|   0.0|\n",
      "|0.3149929018726635|    0.0|               0.0|              0.0|                0.0|         2.887812249678365|        22.895885139634007|22.453403388798698|        9.982893183117008|               0.0|   0.0|\n",
      "|0.3149929018726635|    0.0|               0.0|2.083182920207937|   1.83978528975591|        0.5314021540666731|        23.743880257974645| 23.52457429796754|        7.423177044910995|0.6561951583385822|   0.0|\n",
      "|0.3149929018726635|    0.0|               0.0|              0.0|                0.0|          1.25541377491375|         19.38275964575944| 19.11629268809971|        8.703035114014002|0.9842927375078734|   0.0|\n",
      "|0.3149929018726635|    0.0|               0.0|              0.0|                0.0|         2.887812249678365|        22.895885139634007|22.453403388798698|        9.982893183117008|               0.0|   0.0|\n",
      "|0.3149929018726635|    0.0|               0.0|              0.0|                0.0|          1.25541377491375|         19.38275964575944| 19.11629268809971|        8.703035114014002|0.9842927375078734|   0.0|\n",
      "|0.3149929018726635|    0.0|2.4305252824998718|              0.0|                0.0|        0.5810504917929282|        23.703500124873983|  23.4009778963886|        8.191091764316022|  2.95287821252362|   0.0|\n",
      "|0.3149929018726635|    0.0|               0.0|2.083182920207937|                0.0|        0.5314021540666731|        23.743880257974645| 23.52457429796754|        7.423177044910995|0.6561951583385822|   0.0|\n",
      "|0.3149929018726635|    0.0|               0.0|2.083182920207937|                0.0|         2.036719807646315|        23.017027079339336|22.535801513722497|       10.238864552824056|0.3280975791692911|   0.0|\n",
      "|0.3149929018726635|    0.0|4.8610505649997435|              0.0|                0.0|         2.036719807646315|        23.017027079339336|22.535801513722497|       10.238864552824056|0.3280975791692911|   0.0|\n",
      "|0.3149929018726635|    0.0|               0.0|2.083182920207937|                0.0|        0.5314021540666731|        23.743880257974645| 23.52457429796754|        7.423177044910995|0.6561951583385822|   0.0|\n",
      "|0.3149929018726635|    0.0|               0.0|              0.0|0.20442058775065666|         2.036719807646315|        23.017027079339336|22.535801513722497|       10.238864552824056|0.3280975791692911|   0.0|\n",
      "|0.3149929018726635|    0.0|               0.0|              0.0|                0.0|         2.887812249678365|        22.895885139634007|22.453403388798698|        9.982893183117008|               0.0|   0.0|\n",
      "|0.3149929018726635|    0.0|               0.0|              0.0|                0.0|         1.157613070492394|         22.93626527273467|22.535801513722497|        9.470949833419029|1.3123903166771644|   0.0|\n",
      "|0.3149929018726635|    0.0|2.4305252824998718|              0.0|                0.0|        0.6638019833624167|        23.784261931478646| 23.52457429796754|        7.679148414618043|1.6404878958464555|   0.0|\n",
      "|0.3149929018726635|    0.0|               0.0|2.083182920207937|                0.0|         2.036719807646315|        23.017027079339336|22.535801513722497|       10.238864552824056|0.3280975791692911|   0.0|\n",
      "+------------------+-------+------------------+-----------------+-------------------+--------------------------+--------------------------+------------------+-------------------------+------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "- Convert features into one single vector:\n",
      "+--------------------+------+\n",
      "|            features|Delito|\n",
      "+--------------------+------+\n",
      "|[0.31499290187266...|   0.0|\n",
      "|[0.31499290187266...|   0.0|\n",
      "|[0.31499290187266...|   0.0|\n",
      "|[0.31499290187266...|   0.0|\n",
      "|[0.31499290187266...|   0.0|\n",
      "|[0.31499290187266...|   0.0|\n",
      "|[0.31499290187266...|   0.0|\n",
      "|[0.31499290187266...|   0.0|\n",
      "|[0.31499290187266...|   0.0|\n",
      "|[0.31499290187266...|   0.0|\n",
      "|[0.31499290187266...|   0.0|\n",
      "|[0.31499290187266...|   0.0|\n",
      "|[0.31499290187266...|   0.0|\n",
      "|[0.31499290187266...|   0.0|\n",
      "|[0.31499290187266...|   0.0|\n",
      "|[0.31499290187266...|   0.0|\n",
      "|[0.31499290187266...|   0.0|\n",
      "|[0.31499290187266...|   0.0|\n",
      "|[0.31499290187266...|   0.0|\n",
      "|[0.31499290187266...|   0.0|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "- Building a model using Decision tree classifier.\n",
      "\n",
      "- Building a model using Random Forest Classifier. \n",
      "\n",
      "Step 5: Evaluating Models\n",
      "\n",
      "Make predictions and evaluates the Decision Tree Classifier Model: \n",
      "\n",
      "\n",
      "1. Make predictions with Decision Tree Classifier Model: \n",
      "+------+----------+\n",
      "|Delito|prediction|\n",
      "+------+----------+\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "2. Evaluates Decision Tree Classifier Model: \n",
      "\n",
      "+------+----------+-----------------+\n",
      "|Delito|prediction|      probability|\n",
      "+------+----------+-----------------+\n",
      "|   0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "+------+----------+-----------------+\n",
      "only showing top 15 rows\n",
      "\n",
      "The area under ROC for train data is 0.963355013522\n",
      "The area under ROC for test data is 0.973820354971\n",
      "\n",
      "Make predictions and evaluates the Random Forest Classifier Model: \n",
      "\n",
      "\n",
      "1. Make predictions with Random Forest Classifier Model: \n",
      "+------+----------+\n",
      "|Delito|prediction|\n",
      "+------+----------+\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "2. Evaluates Random Forest Classifier Model: \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+\n",
      "|Delito|prediction|         probability|\n",
      "+------+----------+--------------------+\n",
      "|   0.0|       0.0|[0.88417901373662...|\n",
      "|   0.0|       0.0|[0.88417901373662...|\n",
      "|   0.0|       0.0|[0.88417901373662...|\n",
      "|   0.0|       0.0|[0.88417901373662...|\n",
      "|   0.0|       0.0|[0.88417901373662...|\n",
      "|   0.0|       0.0|[0.88417901373662...|\n",
      "|   0.0|       0.0|[0.88417901373662...|\n",
      "|   0.0|       0.0|[0.88417901373662...|\n",
      "|   0.0|       0.0|[0.88417901373662...|\n",
      "|   0.0|       0.0|[0.88417901373662...|\n",
      "|   0.0|       0.0|[0.88417901373662...|\n",
      "|   0.0|       0.0|[0.88417901373662...|\n",
      "|   0.0|       0.0|[0.88417901373662...|\n",
      "|   0.0|       0.0|[0.88417901373662...|\n",
      "|   0.0|       0.0|[0.88417901373662...|\n",
      "+------+----------+--------------------+\n",
      "only showing top 15 rows\n",
      "\n",
      "The area under ROC for train data is 0.922455636823\n",
      "The area under ROC for test data is 0.938570227192\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    This function extract data and train a model using PySpark\n",
    "    \"\"\"\n",
    "    print(\"Step 1: Entry Data Description\")\n",
    "    show_entry_data_description()\n",
    "    \n",
    "    print(\"Step 2: Data Preprocessing\")\n",
    "    spark_session = create_spark_session()\n",
    "    oij_spark_df, inec_spark_df = data_preprocessing(spark_session)\n",
    "    \n",
    "    print(\"Step 3: Materialization to PostgreSQL\")\n",
    "    materialization_to_postgresql(spark_session,\n",
    "                                  oij_spark_df,\n",
    "                                  inec_spark_df)\n",
    "    \n",
    "    print(\"Step 4: Training Models\")\n",
    "    train_data, test_data = select_data_for_training_models(spark_session)\n",
    "    predict_train_data_1, predict_test_data_1 = training_model_1(train_data, test_data)\n",
    "    predict_train_data_2, predict_test_data_2 = training_model_2(train_data, test_data)\n",
    "    \n",
    "    print(\"Step 5: Evaluating Models\")\n",
    "    evaluating_decision_tree_model(predict_train_data_1, predict_test_data_1)\n",
    "    evaluating_random_forest_model(predict_train_data_2, predict_test_data_2)\n",
    "\n",
    "# Execute main program\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "TP2_BigData_FelipeMejias.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
