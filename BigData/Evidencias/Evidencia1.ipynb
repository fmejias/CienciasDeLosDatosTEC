{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PrimerClaseBigData.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfvwXXdbU-vR",
        "colab_type": "code",
        "outputId": "057123d5-030f-4717-b36f-9670bd071b5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Install necessary libraries\n",
        "\n",
        "!pip3 install pyspark\n",
        "!pip3 install findspark\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-eu.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (2.4.4)\n",
            "Requirement already satisfied: py4j==0.10.7 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.7)\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.6/dist-packages (1.3.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzpNZKlCWKMq",
        "colab_type": "code",
        "outputId": "cc66c56d-50f4-4654-c500-30d1cb8e82b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pandas as pd\n",
        "import findspark\n",
        "findspark.init(\"spark-2.4.4-bin-hadoop2.7\")# SPARK_HOME\n",
        "\n",
        "from datetime import datetime\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, date_format, udf \n",
        "from pyspark.sql.types import DateType\n",
        "\n",
        "# Generate random transactions from a dictionary\n",
        "def generate_transactions():\n",
        "  # Dictionary with all customer data information\n",
        "  customer_data = {\n",
        "      'customer_id' : [1, 1, 1, 1, 1, 2, 2, 2, 2, 2],\n",
        "      'amount' : [55, 125, 32, 64, 128, 333, 334, 333, 11, 44],\n",
        "      'purchased_at' : ['2017-03-01 09:00:00',\n",
        "                        '2017-03-01 10:00:00',\n",
        "                        '2017-03-02 13:00:00',\n",
        "                        '2017-03-02 15:00:00',\n",
        "                        '2017-03-03 10:00:00',\n",
        "                        '2017-03-01 09:00:00',\n",
        "                        '2017-03-01 09:01:00',\n",
        "                        '2017-03-01 09:02:00',\n",
        "                        '2017-03-03 20:00:00',\n",
        "                        '2017-03-03 20:15:00']\n",
        "  }\n",
        "\n",
        "  # Create dataframe from previous dictionary\n",
        "  data_frame = pd.DataFrame.from_dict(customer_data)\n",
        "  return data_frame\n",
        "\n",
        "# Create Spark DataFrame from Pandas DataFrame\n",
        "def create_spark_data_frame(pandas_data_frame):\n",
        "  spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Basic JDBC pipeline\") \\\n",
        "    .getOrCreate()\n",
        "  \n",
        "  # Reading single DataFrame in Spark by retrieving all rows from a DB table.\n",
        "  df = spark.createDataFrame(pandas_data_frame)\n",
        "  df.show()\n",
        "  return df\n",
        "\n",
        "def create_formatted_spark_data_frame(spark_data_frame):\n",
        "  formatted_df = spark_data_frame.withColumn(\"date_string\", date_format(col(\"purchased_at\"), 'MM/dd/yyyy'))\n",
        "  formatted_df.show()\n",
        "  return formatted_df\n",
        "\n",
        "def create_typed_spark_data_frame(formatted_spark_data_frame):\n",
        "  string_to_date = \\\n",
        "    udf(lambda text_date: datetime.strptime(text_date, '%m/%d/%Y'),\n",
        "        DateType())\n",
        "\n",
        "  typed_df = formatted_spark_data_frame.withColumn(\"date\", string_to_date(formatted_spark_data_frame.date_string))\n",
        "  typed_df.show()\n",
        "  typed_df.printSchema()\n",
        "  return typed_df\n",
        "\n",
        "def create_sums_spark_data_frame(typed_spark_data_frame):\n",
        "  sum_df = typed_spark_data_frame.groupBy(\"customer_id\", \"date\").sum()\n",
        "  sum_df.show()\n",
        "  return sum_df\n",
        "\n",
        "def create_stats_spark_data_frame(sum_spark_data_frame):\n",
        "  stats_df = \\\n",
        "    sum_spark_data_frame.select(\n",
        "        col('customer_id'),\n",
        "        col('date'),\n",
        "        col('sum(amount)').alias('amount'))\n",
        "\n",
        "  stats_df.printSchema()\n",
        "  stats_df.show()\n",
        "  return stats_df\n",
        "\n",
        "def create_names_spark_data_frame():\n",
        "  from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n",
        "  names_df = spark \\\n",
        "      .read \\\n",
        "      .format(\"csv\") \\\n",
        "      .option(\"path\", \"names.csv\") \\\n",
        "      .option(\"header\", True) \\\n",
        "      .schema(StructType([\n",
        "                  StructField(\"id\", IntegerType()),\n",
        "                  StructField(\"name\", StringType()),\n",
        "                  StructField(\"currency\", StringType())])) \\\n",
        "      .load()\n",
        "\n",
        "  names_df.printSchema()\n",
        "  names_df.show()\n",
        "  return names_df\n",
        "\n",
        "def create_joint_spark_data_frame(names_spark_data_frame, stats_spark_data_frame):\n",
        "  joint_df = stats_df.join(names_spark_data_frame, stats_spark_data_frame.customer_id == names_spark_data_frame.id)\n",
        "  joint_df.printSchema()\n",
        "  joint_df.show()\n",
        "  return joint_df\n",
        "\n",
        "# Create a dataframe of transactions\n",
        "data_frame = generate_transactions()\n",
        "print(\"Pandas Data Frame: \\n\", data_frame)\n",
        "\n",
        "# Create a Spark DataFrame\n",
        "print(\"\\nSpark Data Frame: \\n\")\n",
        "spark_data_frame = create_spark_data_frame(data_frame)\n",
        "\n",
        "# Formatted Spark DataFrame\n",
        "print(\"\\nFormatted Spark Data Frame: \\n\")\n",
        "formatted_spark_data_frame = create_formatted_spark_data_frame(spark_data_frame)\n",
        "\n",
        "# Create Non Standard Spark Function\n",
        "print(\"\\nTyped Spark Data Frame: \\n\")\n",
        "typed_spark_data_frame = create_typed_spark_data_frame(formatted_spark_data_frame)\n",
        "\n",
        "# Sum Clients Purchases\n",
        "print(\"\\nSum Client Purchases: \\n\")\n",
        "sum_spark_data_frame = create_sums_spark_data_frame(typed_spark_data_frame)\n",
        "\n",
        "# Create Stats Spark DataFrame\n",
        "print(\"\\nStats Spark Data Frame: \\n\")\n",
        "stats_spark_data_frame = create_stats_spark_data_frame(sum_spark_data_frame)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pandas Data Frame: \n",
            "    customer_id  amount         purchased_at\n",
            "0            1      55  2017-03-01 09:00:00\n",
            "1            1     125  2017-03-01 10:00:00\n",
            "2            1      32  2017-03-02 13:00:00\n",
            "3            1      64  2017-03-02 15:00:00\n",
            "4            1     128  2017-03-03 10:00:00\n",
            "5            2     333  2017-03-01 09:00:00\n",
            "6            2     334  2017-03-01 09:01:00\n",
            "7            2     333  2017-03-01 09:02:00\n",
            "8            2      11  2017-03-03 20:00:00\n",
            "9            2      44  2017-03-03 20:15:00\n",
            "\n",
            "Spark Data Frame: \n",
            "\n",
            "+-----------+------+-------------------+\n",
            "|customer_id|amount|       purchased_at|\n",
            "+-----------+------+-------------------+\n",
            "|          1|    55|2017-03-01 09:00:00|\n",
            "|          1|   125|2017-03-01 10:00:00|\n",
            "|          1|    32|2017-03-02 13:00:00|\n",
            "|          1|    64|2017-03-02 15:00:00|\n",
            "|          1|   128|2017-03-03 10:00:00|\n",
            "|          2|   333|2017-03-01 09:00:00|\n",
            "|          2|   334|2017-03-01 09:01:00|\n",
            "|          2|   333|2017-03-01 09:02:00|\n",
            "|          2|    11|2017-03-03 20:00:00|\n",
            "|          2|    44|2017-03-03 20:15:00|\n",
            "+-----------+------+-------------------+\n",
            "\n",
            "\n",
            "Formatted Spark Data Frame: \n",
            "\n",
            "+-----------+------+-------------------+-----------+\n",
            "|customer_id|amount|       purchased_at|date_string|\n",
            "+-----------+------+-------------------+-----------+\n",
            "|          1|    55|2017-03-01 09:00:00| 03/01/2017|\n",
            "|          1|   125|2017-03-01 10:00:00| 03/01/2017|\n",
            "|          1|    32|2017-03-02 13:00:00| 03/02/2017|\n",
            "|          1|    64|2017-03-02 15:00:00| 03/02/2017|\n",
            "|          1|   128|2017-03-03 10:00:00| 03/03/2017|\n",
            "|          2|   333|2017-03-01 09:00:00| 03/01/2017|\n",
            "|          2|   334|2017-03-01 09:01:00| 03/01/2017|\n",
            "|          2|   333|2017-03-01 09:02:00| 03/01/2017|\n",
            "|          2|    11|2017-03-03 20:00:00| 03/03/2017|\n",
            "|          2|    44|2017-03-03 20:15:00| 03/03/2017|\n",
            "+-----------+------+-------------------+-----------+\n",
            "\n",
            "\n",
            "Typed Spark Data Frame: \n",
            "\n",
            "+-----------+------+-------------------+-----------+----------+\n",
            "|customer_id|amount|       purchased_at|date_string|      date|\n",
            "+-----------+------+-------------------+-----------+----------+\n",
            "|          1|    55|2017-03-01 09:00:00| 03/01/2017|2017-03-01|\n",
            "|          1|   125|2017-03-01 10:00:00| 03/01/2017|2017-03-01|\n",
            "|          1|    32|2017-03-02 13:00:00| 03/02/2017|2017-03-02|\n",
            "|          1|    64|2017-03-02 15:00:00| 03/02/2017|2017-03-02|\n",
            "|          1|   128|2017-03-03 10:00:00| 03/03/2017|2017-03-03|\n",
            "|          2|   333|2017-03-01 09:00:00| 03/01/2017|2017-03-01|\n",
            "|          2|   334|2017-03-01 09:01:00| 03/01/2017|2017-03-01|\n",
            "|          2|   333|2017-03-01 09:02:00| 03/01/2017|2017-03-01|\n",
            "|          2|    11|2017-03-03 20:00:00| 03/03/2017|2017-03-03|\n",
            "|          2|    44|2017-03-03 20:15:00| 03/03/2017|2017-03-03|\n",
            "+-----------+------+-------------------+-----------+----------+\n",
            "\n",
            "root\n",
            " |-- customer_id: long (nullable = true)\n",
            " |-- amount: long (nullable = true)\n",
            " |-- purchased_at: string (nullable = true)\n",
            " |-- date_string: string (nullable = true)\n",
            " |-- date: date (nullable = true)\n",
            "\n",
            "\n",
            "Sum Client Purchases: \n",
            "\n",
            "+-----------+----------+----------------+-----------+\n",
            "|customer_id|      date|sum(customer_id)|sum(amount)|\n",
            "+-----------+----------+----------------+-----------+\n",
            "|          1|2017-03-02|               2|         96|\n",
            "|          2|2017-03-01|               6|       1000|\n",
            "|          1|2017-03-01|               2|        180|\n",
            "|          2|2017-03-03|               4|         55|\n",
            "|          1|2017-03-03|               1|        128|\n",
            "+-----------+----------+----------------+-----------+\n",
            "\n",
            "\n",
            "Stats Spark Data Frame: \n",
            "\n",
            "root\n",
            " |-- customer_id: long (nullable = true)\n",
            " |-- date: date (nullable = true)\n",
            " |-- amount: long (nullable = true)\n",
            "\n",
            "+-----------+----------+------+\n",
            "|customer_id|      date|amount|\n",
            "+-----------+----------+------+\n",
            "|          1|2017-03-02|    96|\n",
            "|          2|2017-03-01|  1000|\n",
            "|          1|2017-03-01|   180|\n",
            "|          2|2017-03-03|    55|\n",
            "|          1|2017-03-03|   128|\n",
            "+-----------+----------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}