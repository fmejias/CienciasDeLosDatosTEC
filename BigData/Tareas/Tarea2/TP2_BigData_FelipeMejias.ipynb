{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP2_BigData_FelipeMejias.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fmejias/CienciasDeLosDatosTEC/blob/master/BigData/Tareas/Tarea2/TP2_BigData_FelipeMejias.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoVzbpenBYwm",
        "colab_type": "text"
      },
      "source": [
        "# Big Data\n",
        "# Trabajo práctico 2\n",
        "\n",
        "- Professor: Luis Chavarría.\n",
        "\n",
        "- Student:  \n",
        "    - Felipe Alberto Mejías Loría, Instituto Tecnológico de Costa Rica. \n",
        "\n",
        "- December 05th, 2019"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16-eehY1yTlt",
        "colab_type": "text"
      },
      "source": [
        "## **1-) Instalación de PySpark y Optimus**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfvwXXdbU-vR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install necessary libraries\n",
        "!pip3 install pyspark\n",
        "!pip install -q findspark\n",
        "!pip install optimuspyspark\n",
        "\n",
        "# Needed to install Spark in Google Colab\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-eu.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPYcvcFbykrU",
        "colab_type": "text"
      },
      "source": [
        "# **2-) Actualizar variables de ambiente necesarias para correr Spark en Google Colab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6fZTu6QCCcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set necessary environmental variables to use Apache Spark in Google Colab\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etOFvKZQy3vz",
        "colab_type": "text"
      },
      "source": [
        "# **3-) Importar bibliotecas necesarias para la ejecución de la TP2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr7X3A0ezHGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Necessary Imports for the execution of the TP1\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import findspark\n",
        "import json\n",
        "from datetime import datetime\n",
        "from pyspark.sql import SparkSession, Row, dataframe\n",
        "from pyspark.sql.functions import col, date_format, udf, array\n",
        "from pyspark.sql.types import DateType\n",
        "from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n",
        "from optimus import Optimus\n",
        "from urllib.error import HTTPError\n",
        "from google.colab import files\n",
        "\n",
        "# Set SPARK_HOME. Needed to initialize Apache Spark.\n",
        "findspark.init(\"spark-2.4.4-bin-hadoop2.7\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsVJnOhA0PXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# JSON Files Path\n",
        "CAJA1_JSON_PATH = \"https://raw.githubusercontent.com/fmejias/CienciasDeLosDatosTEC/master/BigData/Tareas/Tarea2/compras_caja1.json\"\n",
        "CAJA2_JSON_PATH = \"https://raw.githubusercontent.com/fmejias/CienciasDeLosDatosTEC/master/BigData/Tareas/Tarea2/compras_caja2.json\"\n",
        "CAJA3_JSON_PATH = \"https://raw.githubusercontent.com/fmejias/CienciasDeLosDatosTEC/master/BigData/Tareas/Tarea2/compras_caja3.json\"\n",
        "CAJA4_JSON_PATH = \"https://raw.githubusercontent.com/fmejias/CienciasDeLosDatosTEC/master/BigData/Tareas/Tarea2/compras_caja4.json\"\n",
        "CAJA5_JSON_PATH = \"https://raw.githubusercontent.com/fmejias/CienciasDeLosDatosTEC/master/BigData/Tareas/Tarea2/compras_caja5.json\"\n",
        "\n",
        "def create_spark_session():\n",
        "  \"\"\"\n",
        "  This function builds a Spark Session\n",
        "  return the main entry of a Spark DataFrame\n",
        "  \"\"\"\n",
        "  spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Basic JDBC pipeline\") \\\n",
        "    .getOrCreate()\n",
        "  return spark\n",
        "\n",
        "def show_complete_spark_data_frame(spark_data_frame):\n",
        "  \"\"\"\n",
        "  This function shows the complete spark_data_frame\n",
        "  \"\"\"\n",
        "  spark_data_frame.show(spark_data_frame.count(), False)\n",
        "\n",
        "def get_column_values_to_list(data_frame, column_name):\n",
        "  \"\"\"\n",
        "  This function returns the values of a column into a list\n",
        "  data_frame: Spark DataFrame\n",
        "  column_name: Column Name to get the values from\n",
        "  \"\"\"\n",
        "  return data_frame.select(column_name).rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "def print_formatted_dictionary(dictionary):\n",
        "  \"\"\"\n",
        "  This function shows the content of a Python Dictionary\n",
        "  dictionary: Python Dictionary from Pandas DataFrame\n",
        "  \"\"\"\n",
        "  print(json.dumps(dictionary, indent = 4), \"\\n\")\n",
        "\n",
        "def load_json_files_to_dict():\n",
        "  \"\"\"\n",
        "  This function loads JSON Files into Python Dictionaries\n",
        "  return list with all dictionaries\n",
        "  \"\"\"\n",
        "  # Load JSON Files in Pandas DataFrame and then traslated to dictionary\n",
        "  caja1_dict = pd.read_json(CAJA1_JSON_PATH, orient='columns').to_dict('records')\n",
        "  caja2_dict = pd.read_json(CAJA2_JSON_PATH, orient='columns').to_dict('records')\n",
        "  caja3_dict = pd.read_json(CAJA3_JSON_PATH, orient='columns').to_dict('records')\n",
        "  caja4_dict = pd.read_json(CAJA4_JSON_PATH, orient='columns').to_dict('records')\n",
        "  caja5_dict = pd.read_json(CAJA5_JSON_PATH, orient='columns').to_dict('records')\n",
        "\n",
        "  return [caja1_dict, caja2_dict, caja3_dict, caja4_dict, caja5_dict]\n",
        "\n",
        "def create_spark_data_frame_sales_per_caja_row(total_sold, caja_id):\n",
        "  \"\"\"\n",
        "  This function creates a Spark Data Frame Row\n",
        "  total_sold: total amount sold per caja\n",
        "  caja_id: Sale identifier\n",
        "  return the Product Row\n",
        "  \"\"\"\n",
        "  total_sold_row = Row(\"caja\", \"total_vendido\")\n",
        "  return total_sold_row(caja_id, total_sold)\n",
        "\n",
        "def create_spark_data_frame_metric_row(metric_type, metric_value):\n",
        "  \"\"\"\n",
        "  This function creates a Spark Data Frame Row\n",
        "  metric_type: type of metric\n",
        "  metric_value: value of metric\n",
        "  return the Metric Row\n",
        "  \"\"\"\n",
        "  metric_row = Row(\"tipo_metrica\", \"valor\")\n",
        "  return metric_row(metric_type, metric_value)\n",
        "\n",
        "def create_spark_data_frame_product_row(product_dict, caja_id):\n",
        "  \"\"\"\n",
        "  This function creates a Spark Data Frame Row\n",
        "  product_dict: Python Dictionary with the information of a Product\n",
        "  caja_id: Sale identifier\n",
        "  return the Product Row\n",
        "  \"\"\"\n",
        "  product_row = Row(\"caja\", \"nombre\", \"cantidad\", \"precio_unitario\")\n",
        "  return product_row(caja_id, product_dict[\"nombre\"], product_dict[\"cantidad\"],\n",
        "                     product_dict[\"precio_unitario\"])\n",
        "\n",
        "def create_spark_data_frame_from_rows_list(rows_list, columns_name_list):\n",
        "  \"\"\"\n",
        "  This function creates a Spark Data Frame from a list of rows\n",
        "  rows_list: List with all rows for the new Data Frame\n",
        "  columns_name_list: List with the names of the columns\n",
        "  return the Spark Data Frame\n",
        "  \"\"\"\n",
        "  spark = create_spark_session()\n",
        "  spark_data_frame = spark.createDataFrame(rows_list, columns_name_list)\n",
        "  return spark_data_frame\n",
        "\n",
        "def set_total_sold_per_caja_spark_data_frame_row(sales_dict):\n",
        "  \"\"\"\n",
        "  This function creates a row with Total Sold Per Caja\n",
        "  sales_dict: Dictionary with all sales of a caja\n",
        "  return the Row with Total Sold Per Caja\n",
        "  \"\"\"\n",
        "  total_sold_amount = 0\n",
        "  caja_id = 0\n",
        "  for sale in sales_dict:\n",
        "    if \"compras\" in sale and sale[\"compras\"]:\n",
        "      for product in sale[\"compras\"]:\n",
        "        total_sold_amount += int(product[\"cantidad\"])*int(product[\"precio_unitario\"])\n",
        "        caja_id = sale[\"numero_caja\"]\n",
        "  return create_spark_data_frame_sales_per_caja_row(total_sold_amount, caja_id)\n",
        "\n",
        "def create_list_with_product_spark_data_frame_rows(sales_dict):\n",
        "  \"\"\"\n",
        "  This function creates a List with Product Rows\n",
        "  sales_dict: Dictionary with all sales of a caja\n",
        "  return the List with Product Rows\n",
        "  \"\"\"\n",
        "  products_rows = []\n",
        "  for sale in sales_dict:\n",
        "    if \"compras\" in sale and sale[\"compras\"]:\n",
        "      for product in sale[\"compras\"]:\n",
        "        product_row = create_spark_data_frame_product_row(product,\n",
        "                                                          sale[\"numero_caja\"])\n",
        "        products_rows.append(product_row)\n",
        "  return products_rows\n",
        "\n",
        "def create_spark_data_frame_from_dict(sales_dict):\n",
        "  \"\"\"\n",
        "  This function creates a Spark Data Frame from a sales dictionary\n",
        "  sales_dict: Dictionary with all sales of a caja\n",
        "  return the Product Spark Data Frame\n",
        "  \"\"\"\n",
        "  # Create products rows\n",
        "  products_rows = create_list_with_product_spark_data_frame_rows(sales_dict)\n",
        "  \n",
        "  # Create Product Spark Data Frame\n",
        "  products_df = create_spark_data_frame_from_rows_list(products_rows,\n",
        "                                                       ['caja', 'nombre',\n",
        "                                                        'cantidad',\n",
        "                                                        'precio_unitario'])\n",
        "  products_df = products_df.withColumn(\"cantidad\", products_df[\"cantidad\"].cast(IntegerType()))\n",
        "  return products_df\n",
        "  \n",
        "\n",
        "def create_spark_data_frame_list_from_sales_list(sales_list):\n",
        "  \"\"\"\n",
        "  This function creates a Spark Data Frame List from a list of sales dictionaries\n",
        "  sales_list: List with all sales of a caja\n",
        "  return the List with all Product Spark Data Frames\n",
        "  \"\"\"\n",
        "  spark_df_list = []\n",
        "  for sales_dict in sales_list:\n",
        "    product_data_frame = create_spark_data_frame_from_dict(sales_dict)\n",
        "    spark_df_list.append(product_data_frame)\n",
        "  return spark_df_list\n",
        "\n",
        "def create_spark_data_frame_from_total_sold_rows_list(sales_list):\n",
        "  \"\"\"\n",
        "  This function creates a Spark Data Frame List from a list of sales dictionaries\n",
        "  sales_list: List with all sales of a caja\n",
        "  return the List with all Product Spark Data Frames\n",
        "  \"\"\"\n",
        "  spark_df_total_sold_rows_list = []\n",
        "  for sales_dict in sales_list:\n",
        "    total_sold_row = set_total_sold_per_caja_spark_data_frame_row(sales_dict)\n",
        "    spark_df_total_sold_rows_list.append(total_sold_row)\n",
        "  \n",
        "  # Create Total Sold Per Caja Spark Data Frame\n",
        "  total_sold_df = create_spark_data_frame_from_rows_list(spark_df_total_sold_rows_list,\n",
        "                                                        ['caja',\n",
        "                                                        'total_vendido'])\n",
        "  return total_sold_df\n",
        "\n",
        "def create_union_products_data_frame(spark_products_df_list):\n",
        "  \"\"\"\n",
        "  This function calculates the union of products Data Frames\n",
        "  spark_products_df_list: Spark Data Frames List\n",
        "  return the union of Product Data Frames\n",
        "  \"\"\"\n",
        "  union_product_df = None\n",
        "  while len(spark_products_df_list) >= 1:\n",
        "    if union_product_df is None and len(spark_products_df_list) == 1:\n",
        "      break\n",
        "    elif union_product_df is None:\n",
        "      union_product_df = spark_products_df_list[0].union(spark_products_df_list[1])\n",
        "      spark_products_df_list.pop(1)\n",
        "      spark_products_df_list.pop(0)\n",
        "    else:\n",
        "      union_product_df = union_product_df.union(spark_products_df_list[0])\n",
        "      spark_products_df_list.pop(0)\n",
        "  \n",
        "  return union_product_df if union_product_df is not None else spark_products_df_list[0]\n",
        "\n",
        "\n",
        "def set_total_products_data_frame(union_products_df):\n",
        "  \"\"\"\n",
        "  This function calculates the total of sold products\n",
        "  union_products_df: Spark Data Frame with all products\n",
        "  return the Data Frame Order by Products and amount of sales of each product\n",
        "  \"\"\"\n",
        "  group_by_name_df = union_products_df.groupBy(\"nombre\").sum()\n",
        "  group_by_name_df = group_by_name_df.select(col('nombre'),\n",
        "                                             col('sum(cantidad)').alias('cantidad_vendida'))\n",
        "  return group_by_name_df\n",
        "\n",
        "\n",
        "def calculate_total_of_products(sales_list):\n",
        "  \"\"\"\n",
        "  This function calculates the CSV file with the total of products\n",
        "  sales_list: List with all sales of a caja\n",
        "  return the CSV file\n",
        "  \"\"\"\n",
        "  spark_products_df_list = create_spark_data_frame_list_from_sales_list(sales_list)\n",
        "  union_products_df = create_union_products_data_frame(spark_products_df_list)\n",
        "  total_products_df = set_total_products_data_frame(union_products_df)\n",
        "\n",
        "  # Show Total Products Data Frame\n",
        "  show_complete_spark_data_frame(total_products_df)\n",
        "\n",
        "  # Generate CSV File\n",
        "  return_output_csv_files(total_products_df, 'total_productos')\n",
        "\n",
        "  return total_products_df\n",
        "\n",
        "def calculate_sold_products_per_caja(sales_list):\n",
        "  \"\"\"\n",
        "  This function calculates the CSV file with the sold products per caja\n",
        "  sales_list: List with all sales of a caja\n",
        "  return the CSV file\n",
        "  \"\"\"\n",
        "  total_sold_products_df = create_spark_data_frame_from_total_sold_rows_list(sales_list)\n",
        "\n",
        "  # Show Total Products Data Frame\n",
        "  show_complete_spark_data_frame(total_sold_products_df)\n",
        "\n",
        "  # Generate CSV File\n",
        "  return_output_csv_files(total_sold_products_df, 'total_cajas')\n",
        "\n",
        "  return total_sold_products_df\n",
        "\n",
        "def get_product_with_more_sales(total_products_df):\n",
        "  \"\"\"\n",
        "  This function returns the row with product with more sales metric\n",
        "  total_products_df: DataFrame with total products\n",
        "  return the Spark Row\n",
        "  \"\"\"\n",
        "  # Order Sold Products\n",
        "  order_total_products_df = total_products_df.orderBy(total_products_df.cantidad_vendida.desc())\n",
        "  product_with_more_sales = order_total_products_df.head().nombre\n",
        "\n",
        "  return create_spark_data_frame_metric_row(\"producto_mas_vendido_por_unidad\",\n",
        "                                            str(product_with_more_sales))\n",
        "\n",
        "def get_caja_with_more_sales(total_sold_products_df):\n",
        "  \"\"\"\n",
        "  This function returns the row with more sales metric\n",
        "  total_sold_products_df: DataFrame with total sold products\n",
        "  return the Spark Row\n",
        "  \"\"\"\n",
        "  # Order Sold Products\n",
        "  order_sold_products_df = total_sold_products_df.orderBy(total_sold_products_df.total_vendido.desc())\n",
        "  caja_with_more_sales = order_sold_products_df.head().caja\n",
        "\n",
        "  return create_spark_data_frame_metric_row(\"caja_con_mas_ventas\", str(caja_with_more_sales))\n",
        "\n",
        "def get_caja_with_less_sales(total_sold_products_df):\n",
        "  \"\"\"\n",
        "  This function returns the row with less sales metric\n",
        "  total_sold_products_df: DataFrame with total sold products\n",
        "  return the Spark Row\n",
        "  \"\"\"\n",
        "  # Order Sold Products\n",
        "  order_sold_products_df = total_sold_products_df.orderBy(total_sold_products_df.total_vendido.asc())\n",
        "  caja_with_less_sales = order_sold_products_df.head().caja\n",
        "\n",
        "  return create_spark_data_frame_metric_row(\"caja_con_menos_ventas\", str(caja_with_less_sales))\n",
        "\n",
        "def get_percentile(total_sold_products_df, percentile_number=25):\n",
        "  \"\"\"\n",
        "  This function returns the row with percentile metric\n",
        "  total_sold_products_df: DataFrame with total sold products\n",
        "  percentile_number: Percentile value to calculate\n",
        "  return the Spark Row\n",
        "  \"\"\"\n",
        "  # Order Sold Products\n",
        "  order_sold_products_df = total_sold_products_df.orderBy(total_sold_products_df.total_vendido.asc())\n",
        "\n",
        "  # Get total_vendido\n",
        "  list_of_total_sold = get_column_values_to_list(order_sold_products_df,\n",
        "                                                 'total_vendido')\n",
        "  \n",
        "  # Get Percentile Value\n",
        "  percentile_value = int(np.percentile(np.array(list_of_total_sold), percentile_number))\n",
        "  percentile_column_name = \"percentil_{percentile_number}_por_caja\".format(percentile_number = percentile_number)\n",
        "  return create_spark_data_frame_metric_row(percentile_column_name, str(percentile_value))\n",
        "\n",
        "def get_higher_income_product(sales_list):\n",
        "  \"\"\"\n",
        "  This function returns the row with higher income product metric\n",
        "  sales_list: List with all sales\n",
        "  return the Spark Row\n",
        "  \"\"\"\n",
        "  spark_products_df_list = create_spark_data_frame_list_from_sales_list(sales_list)\n",
        "  union_products_df = create_union_products_data_frame(spark_products_df_list)\n",
        "  \n",
        "  # Multiply amount and quantity \n",
        "  products_order_by_name_df = union_products_df.orderBy(\"nombre\")\n",
        "  amount_times_price_op = products_order_by_name_df['cantidad']*products_order_by_name_df['precio_unitario']\n",
        "\n",
        "  # Add Column with CantidadxPrecio and the select only Name and CantidadxPrecio\n",
        "  products_with_income_df = products_order_by_name_df.withColumn('CantidadxPrecio',\n",
        "                                                                 amount_times_price_op)\n",
        "  products_select_income_and_name_df = products_with_income_df.select(\"nombre\",\n",
        "                                                                      \"CantidadxPrecio\")\n",
        "  \n",
        "  # Group products income by name and the order by desc\n",
        "  products_group_by_income_df = products_select_income_and_name_df.groupBy(\"nombre\").sum()\n",
        "  products_group_by_amount_sum_df = products_group_by_income_df.select(col('nombre'),\n",
        "                                                                       col('sum(CantidadxPrecio)').alias('ingresos_generados'))\n",
        "  products_group_by_amount_sum_df = products_group_by_amount_sum_df.orderBy(products_group_by_amount_sum_df.ingresos_generados.desc())\n",
        "\n",
        "  # Get higher income product and create metric row\n",
        "  higher_income_product = products_group_by_amount_sum_df.head().nombre\n",
        "  return create_spark_data_frame_metric_row(\"producto_de_mayor_ingreso\", str(higher_income_product))\n",
        "\n",
        "  \n",
        "\n",
        "def calculate_sales_metrics(sales_dicts_list, total_products_df,\n",
        "                            total_sold_products_df):\n",
        "  \"\"\"\n",
        "  This function calculates the CSV file with the sales metrics\n",
        "  total_products_df: DataFrame with total products \n",
        "  total_sold_products_df: DataFrame with total sold products\n",
        "  return the CSV file\n",
        "  \"\"\"\n",
        "  caja_with_more_sales_row = get_caja_with_more_sales(total_sold_products_df)\n",
        "  caja_with_less_sales_row = get_caja_with_less_sales(total_sold_products_df)\n",
        "  percentile_25_row = get_percentile(total_sold_products_df, percentile_number=25)\n",
        "  percentile_50_row = get_percentile(total_sold_products_df, percentile_number=50)\n",
        "  percentile_75_row = get_percentile(total_sold_products_df, percentile_number=75)\n",
        "  product_with_more_sales_row = get_product_with_more_sales(total_products_df)\n",
        "  higher_income_product_row = get_higher_income_product(sales_dicts_list)\n",
        "\n",
        "  metrics_df = create_spark_data_frame_from_rows_list([caja_with_more_sales_row,\n",
        "                                                       caja_with_less_sales_row,\n",
        "                                                       percentile_25_row,\n",
        "                                                       percentile_50_row,\n",
        "                                                       percentile_75_row,\n",
        "                                                       product_with_more_sales_row,\n",
        "                                                       higher_income_product_row],\n",
        "                                                      ['tipo_metrica',\n",
        "                                                       'valor'])\n",
        "  # Show Total Products Data Frame\n",
        "  show_complete_spark_data_frame(metrics_df)\n",
        "\n",
        "  # Generate CSV File\n",
        "  return_output_csv_files(metrics_df, 'metricas')\n",
        "\n",
        "def return_output_csv_files(spark_data_frame, filename):\n",
        "  \"\"\"\n",
        "  This function creates the CSV file from a Spark Data Frame\n",
        "  return the CSV file\n",
        "  \"\"\"\n",
        "  from google.colab import files\n",
        "  pandas_df = spark_data_frame.select(\"*\").toPandas()\n",
        "  pandas_df.to_csv('{filename}.csv'.format(filename=filename), index=False) \n",
        "  files.download('{filename}.csv'.format(filename=filename))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Qz_1dBJJgFd",
        "colab_type": "text"
      },
      "source": [
        "# **5-) Funciones principales del programa y función main() para ejecutar el programa principal**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSp6jIOgBXzs",
        "colab_type": "code",
        "outputId": "0e7b9ed2-9437-41c8-acba-695cae28b9a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def main():\n",
        "  \"\"\"\n",
        "  This function calculates the products in a Supermarket\n",
        "  \"\"\"\n",
        "  # Create Python Dictionaries from JSON Files\n",
        "  sales_dicts_list = load_json_files_to_dict()\n",
        "\n",
        "  # Calculate Total of Products\n",
        "  print(\"\\nEl total de productos vendidos en todas las cajas es: \\n\")\n",
        "  total_products_df = calculate_total_of_products(sales_dicts_list)\n",
        "\n",
        "  # Calculate Total of Cajas\n",
        "  print(\"\\nEl total de productos vendidos por caja es: \\n\")\n",
        "  total_sold_products_df = calculate_sold_products_per_caja(sales_dicts_list)\n",
        "\n",
        "  # Calculate Metrics\n",
        "  print(\"\\nLas estadísticas de las ventas son las siguientes: \\n\")\n",
        "  calculate_sales_metrics(sales_dicts_list, total_products_df,\n",
        "                          total_sold_products_df)\n",
        "\n",
        "# Execute main program\n",
        "main()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "El total de productos vendidos en todas las cajas es: \n",
            "\n",
            "+-------------------+----------------+\n",
            "|nombre             |cantidad_vendida|\n",
            "+-------------------+----------------+\n",
            "|Helado Cero Grados |15              |\n",
            "|FrescoLeche        |7               |\n",
            "|Pepino             |7               |\n",
            "|durazno            |16              |\n",
            "|Chocoleta          |18              |\n",
            "|brocoli            |10              |\n",
            "|Cebolla            |8               |\n",
            "|Leche              |13              |\n",
            "|Cremoleta          |18              |\n",
            "|Pollo adobado      |5               |\n",
            "|Azucar             |3               |\n",
            "|Salchichon         |3               |\n",
            "|Prestobarba        |4               |\n",
            "|Pringles           |10              |\n",
            "|Papel higienico    |2               |\n",
            "|Chocolate          |12              |\n",
            "|Mantequilla de Maní|4               |\n",
            "|Mejitos            |10              |\n",
            "|fresas             |37              |\n",
            "|Pan Bimbo          |4               |\n",
            "|papas              |42              |\n",
            "|Cepillo de dientes |2               |\n",
            "|Tronaditas         |14              |\n",
            "|manzana            |17              |\n",
            "|ciruelas           |19              |\n",
            "|Apio               |6               |\n",
            "|pavo               |5               |\n",
            "|Hilo Dental        |1               |\n",
            "|Frijoles Molidos   |5               |\n",
            "|limon              |29              |\n",
            "|Salchicha          |10              |\n",
            "|aguacate           |7               |\n",
            "|Fideos             |2               |\n",
            "|Pasta Dental       |1               |\n",
            "|Shampoo            |2               |\n",
            "|Picaritas          |12              |\n",
            "|Jalea de Guayaba   |1               |\n",
            "|Nutella            |2               |\n",
            "|coliflor           |5               |\n",
            "|Carne adobada      |5               |\n",
            "|Helado Deleite     |4               |\n",
            "|Chile Dulce        |4               |\n",
            "+-------------------+----------------+\n",
            "\n",
            "\n",
            "El total de productos vendidos por caja es: \n",
            "\n",
            "+----+-------------+\n",
            "|caja|total_vendido|\n",
            "+----+-------------+\n",
            "|1   |60362        |\n",
            "|2   |125224       |\n",
            "|3   |103406       |\n",
            "|4   |73735        |\n",
            "|5   |59562        |\n",
            "+----+-------------+\n",
            "\n",
            "\n",
            "Las estadísticas de las ventas son las siguientes: \n",
            "\n",
            "+-------------------------------+------+\n",
            "|tipo_metrica                   |valor |\n",
            "+-------------------------------+------+\n",
            "|caja_con_mas_ventas            |2     |\n",
            "|caja_con_menos_ventas          |5     |\n",
            "|percentil_25_por_caja          |60362 |\n",
            "|percentil_50_por_caja          |73735 |\n",
            "|percentil_75_por_caja          |103406|\n",
            "|producto_mas_vendido_por_unidad|papas |\n",
            "|producto_de_mayor_ingreso      |pavo  |\n",
            "+-------------------------------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmMp47oJWrty",
        "colab_type": "text"
      },
      "source": [
        "# **6-) Pruebas Unitarias con Pytest**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAGVvWuUXBgO",
        "colab_type": "text"
      },
      "source": [
        "**6.1) Instalar Pytest en Google Colab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQ-NULniW0Wy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install ipytest\n",
        "!pip install pytest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MkdGdL4XQd5",
        "colab_type": "text"
      },
      "source": [
        "**6.2) Importar Pytest y los comandos llamados magics para lograr correr Pytest en Google Colab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1277XV1bXcDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ipytest.magics\n",
        "import pytest\n",
        "import sys\n",
        "from pytest import fixture,mark\n",
        "\n",
        "# This is needed in order to fix the __file__ issue that Google Colab throws\n",
        "__file__ = sys.argv[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCjHZ_jLMxl-",
        "colab_type": "text"
      },
      "source": [
        "**6.3) Datos utilitarios para las pruebas unitarias**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsinUdelM-NU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create Spark Session to convert Pandas Dataframe to Spark Dataframe\n",
        "spark = create_spark_session()\n",
        "\n",
        "# Expected Products per Sale\n",
        "expected_products_per_sale = [\n",
        "    {'caja': \"1\",\n",
        "     'producto': {\"nombre\": \"Mejitos\", \"cantidad\": \"1\", \"precio_unitario\": \"100\"}},\n",
        "    {'caja': \"2\",\n",
        "     'producto': {\"nombre\": \"Chocoleta\", \"cantidad\": \"2\", \"precio_unitario\": \"150\"}}\n",
        "]\n",
        "\n",
        "# Expected Products Sales Per Caja\n",
        "expected_products_sales_per_caja = [\n",
        "{\"total_productos\": 5,\n",
        " \"compras\": [\n",
        "{\n",
        "    \"numero_caja\": \"1\",\n",
        "    \"compras\": [\n",
        "                {\n",
        "                    \"nombre\": \"manzana\",\n",
        "                    \"cantidad\": \"3\",\n",
        "                    \"precio_unitario\": \"22\"\n",
        "                },\n",
        "                {\n",
        "                    \"nombre\": \"brocoli\",\n",
        "                    \"cantidad\": \"2\",\n",
        "                    \"precio_unitario\": \"33\"\n",
        "                }\n",
        "    ]\n",
        "},\n",
        "{\n",
        "    \"numero_caja\": \"1\",\n",
        "    \"compras\": [\n",
        "                {\n",
        "                    \"nombre\": \"Chocoleta\",\n",
        "                    \"cantidad\": \"3\",\n",
        "                    \"precio_unitario\": \"22\"\n",
        "                },\n",
        "                {\n",
        "                    \"nombre\": \"Cremoleta\",\n",
        "                    \"cantidad\": \"2\",\n",
        "                    \"precio_unitario\": \"33\"\n",
        "                },\n",
        "                {\n",
        "                    \"nombre\": \"Chocolate\",\n",
        "                    \"cantidad\": \"4\",\n",
        "                    \"precio_unitario\": \"3300\"\n",
        "                }\n",
        "    ]\n",
        "}]},\n",
        "{\"total_productos\": 5,\n",
        " \"compras\": [\n",
        " {\n",
        "    \"numero_caja\": \"2\",\n",
        "    \"compras\": [\n",
        "                {\n",
        "                    \"nombre\": \"manzana\",\n",
        "                    \"cantidad\": \"3\",\n",
        "                    \"precio_unitario\": \"22\"\n",
        "                },\n",
        "                {\n",
        "                    \"nombre\": \"brocoli\",\n",
        "                    \"cantidad\": \"2\",\n",
        "                    \"precio_unitario\": \"33\"\n",
        "                }\n",
        "    ]\n",
        "},\n",
        "{\n",
        "    \"numero_caja\": \"2\",\n",
        "    \"compras\": [\n",
        "                {\n",
        "                    \"nombre\": \"manzana\",\n",
        "                    \"cantidad\": \"3\",\n",
        "                    \"precio_unitario\": \"22\"\n",
        "                },\n",
        "                {\n",
        "                    \"nombre\": \"brocoli\",\n",
        "                    \"cantidad\": \"2\",\n",
        "                    \"precio_unitario\": \"33\"\n",
        "                },\n",
        "                {\n",
        "                    \"nombre\": \"pavo\",\n",
        "                    \"cantidad\": \"3\",\n",
        "                    \"precio_unitario\": \"3300\"\n",
        "                }\n",
        "    ]\n",
        "}\n",
        "]}]\n",
        "\n",
        "# Expected Products DataFrame\n",
        "expected_products_df_dict = {\n",
        "    'caja': [\"1\", \"1\", \"1\", \"1\", \"1\"],\n",
        "    'nombre': [\"manzana\", \"brocoli\",\n",
        "               \"Chocoleta\", \"Cremoleta\",\n",
        "               \"Chocolate\"],\n",
        "    'cantidad': [\"3\", \"2\", \"3\", \"2\", \"4\"],\n",
        "    'precio_unitario': [\"22\", \"33\", \"22\", \"33\", \"3300\"]\n",
        "}\n",
        "\n",
        "# Expected Final Total Products DataFrame\n",
        "expected_total_products_df_dict = {\n",
        "    'nombre': [\"manzana\", \"brocoli\",\n",
        "               \"Chocoleta\", \"Cremoleta\",\n",
        "               \"Chocolate\"],\n",
        "    'cantidad_vendida': [\"3\", \"2\", \"3\", \"2\", \"4\"]\n",
        "}\n",
        "\n",
        "# Expected Final Total Sold Products DataFrame\n",
        "expected_total_sold_products_df_dict = {\n",
        "    'caja': ['1'],\n",
        "    'total_vendido': [\"13464\"]\n",
        "}\n",
        "\n",
        "# Expected Total Sold Products\n",
        "expected_total_sold_products = [\n",
        "    {'caja': 1,\n",
        "     'total_vendido': 15000},\n",
        "    {'caja': 2,\n",
        "     'total_vendido': 17000}\n",
        "]\n",
        "\n",
        "# Expected Sales Metrics\n",
        "expected_sales_metrics = [\n",
        "    {'tipo_metrica': 'caja_con_mas_ventas',\n",
        "     'valor': '1'},\n",
        "    {'tipo_metrica': 'caja_con_menos_ventas',\n",
        "     'valor': '5'},\n",
        "    {'tipo_metrica': 'percentil_25_por_caja',\n",
        "     'valor': '1500'},\n",
        "    {'tipo_metrica': 'percentil_50_por_caja',\n",
        "     'valor': '2500'},\n",
        "    {'tipo_metrica': 'percentil_75_por_caja',\n",
        "     'valor': '3500'},\n",
        "    {'tipo_metrica': 'producto_mas_vendido_por_unidad',\n",
        "     'valor': 'pavo'},\n",
        "    {'tipo_metrica': 'producto_de_mayor_ingreso',\n",
        "     'valor': 'apio'}\n",
        "]\n",
        "\n",
        "def convert_from_dict_to_spark(expected_dict):\n",
        "  pandas_df = pd.DataFrame.from_dict(expected_dict)\n",
        "  return spark.createDataFrame(pandas_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzONRVcqXtrm",
        "colab_type": "text"
      },
      "source": [
        "**6.4) Pruebas unitarias para obtener el total de productos**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UypeUstPX-E6",
        "colab_type": "code",
        "outputId": "18db4ce1-9dda-4f65-8bcc-bd034d074e67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "source": [
        "# This command is needed to run the UTs in Google Colab\n",
        "%%run_pytest[clean] -s\n",
        "\n",
        "@fixture(scope=\"module\")\n",
        "def total_products_functionality_fixture():\n",
        "    # Convert Pandas DataFrame to Spark DataFrame\n",
        "    global expected_products_spark_df\n",
        "    expected_products_spark_df = convert_from_dict_to_spark(expected_products_df_dict)\n",
        "    assert expected_products_spark_df is not None, \\\n",
        "          'Error when created the expected DataFrame'\n",
        "    \n",
        "    global expected_total_products_spark_df\n",
        "    expected_total_products_spark_df = convert_from_dict_to_spark(expected_total_products_df_dict)\n",
        "    assert expected_total_products_spark_df is not None, \\\n",
        "          'Error when created the expected DataFrame'\n",
        "\n",
        "def test_create_succesful_spark_session():\n",
        "    assert create_spark_session() is not None\n",
        "\n",
        "@mark.parametrize('product_per_sale', expected_products_per_sale)\n",
        "def test_create_spark_data_frame_product_row(product_per_sale):\n",
        "    product_spark_row = create_spark_data_frame_product_row(product_per_sale[\"producto\"],\n",
        "                                                            product_per_sale[\"caja\"])\n",
        "    assert product_spark_row is not None\n",
        "    assert isinstance(product_spark_row, Row)\n",
        "    assert [\"caja\", \"nombre\", \"cantidad\", \"precio_unitario\"] == list(product_spark_row.asDict())\n",
        "    assert product_per_sale[\"caja\"] == product_spark_row[\"caja\"]\n",
        "    assert product_per_sale[\"producto\"][\"nombre\"] == product_spark_row[\"nombre\"]\n",
        "    assert product_per_sale[\"producto\"][\"cantidad\"] == product_spark_row[\"cantidad\"]\n",
        "    assert product_per_sale[\"producto\"][\"precio_unitario\"] == product_spark_row[\"precio_unitario\"]\n",
        "\n",
        "@mark.parametrize('sale_per_caja', expected_products_sales_per_caja)\n",
        "def test_create_list_with_product_spark_data_frame_rows(sale_per_caja):\n",
        "  products_rows_list = create_list_with_product_spark_data_frame_rows(sale_per_caja[\"compras\"])\n",
        "  assert len(products_rows_list) == sale_per_caja[\"total_productos\"]\n",
        "\n",
        "def test_create_spark_data_frame_from_rows_list(total_products_functionality_fixture):\n",
        "  sales_to_test = expected_products_sales_per_caja[0][\"compras\"]\n",
        "  products_rows_list = create_list_with_product_spark_data_frame_rows(sales_to_test)\n",
        "  products_df = create_spark_data_frame_from_rows_list(products_rows_list,\n",
        "                                                       ['caja', 'nombre',\n",
        "                                                        'cantidad',\n",
        "                                                        'precio_unitario'])\n",
        "  dataframes_difference = products_df.exceptAll(expected_products_spark_df)\n",
        "\n",
        "  # Check the resulting DataFrame from the difference has no rows as there\n",
        "  # is no difference between DataFrames\n",
        "  assert dataframes_difference.count() == 0\n",
        "\n",
        "def test_create_spark_data_frame_from_dict(total_products_functionality_fixture):\n",
        "  sales_to_test = expected_products_sales_per_caja[0][\"compras\"]\n",
        "  products_df = create_spark_data_frame_from_dict(sales_to_test)\n",
        "\n",
        "  dataframes_difference = products_df.exceptAll(expected_products_spark_df)\n",
        "\n",
        "  # Check the resulting DataFrame from the difference has no rows as there\n",
        "  # is no difference between DataFrames\n",
        "  assert dataframes_difference.count() == 0\n",
        "\n",
        "\n",
        "def test_create_spark_data_frame_list_from_sales_list(total_products_functionality_fixture):\n",
        "  sales_list_to_test = [list(expected_products_sales_per_caja[0][\"compras\"])]\n",
        "  products_df_list = create_spark_data_frame_list_from_sales_list(sales_list_to_test)\n",
        "  assert len(products_df_list) == 1\n",
        "  \n",
        "  # Get Product DataFrame from list\n",
        "  products_df = products_df_list[0]\n",
        "  dataframes_difference = products_df.exceptAll(expected_products_spark_df)\n",
        "\n",
        "  # Check the resulting DataFrame from the difference has no rows as there\n",
        "  # is no difference between DataFrames\n",
        "  assert dataframes_difference.count() == 0\n",
        "\n",
        "def test_calculate_total_of_products(total_products_functionality_fixture):\n",
        "  sales_list_to_test = [list(expected_products_sales_per_caja[0][\"compras\"])]\n",
        "  total_products_df = calculate_total_of_products(sales_list_to_test)\n",
        "\n",
        "  # Get Product DataFrame from list\n",
        "  dataframes_difference = total_products_df.exceptAll(expected_total_products_spark_df)\n",
        "\n",
        "  # Check the resulting DataFrame from the difference has no rows as there\n",
        "  # is no difference between DataFrames\n",
        "  assert dataframes_difference.count() == 0\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================= test session starts ==============================\n",
            "platform linux -- Python 3.6.8, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "rootdir: /usr/local, inifile: setup.cfg\n",
            "collected 9 items\n",
            "\n",
            "../usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py ........+---------+----------------+\n",
            "|nombre   |cantidad_vendida|\n",
            "+---------+----------------+\n",
            "|Chocoleta|3               |\n",
            "|brocoli  |2               |\n",
            "|Cremoleta|2               |\n",
            "|Chocolate|4               |\n",
            "|manzana  |3               |\n",
            "+---------+----------------+\n",
            "\n",
            ".\n",
            "\n",
            "========================== 9 passed in 12.74 seconds ===========================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jYtf1A6gzPp",
        "colab_type": "text"
      },
      "source": [
        "**6.5) Pruebas unitarias para obtener el total de las cajas**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHqR7ohHg8J7",
        "colab_type": "code",
        "outputId": "258fe64f-1a4f-4767-f6c7-24de2123a5e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "# This command is needed to run the UTs in Google Colab\n",
        "%%run_pytest[clean] -s\n",
        "\n",
        "@fixture(scope=\"module\")\n",
        "def total_sold_products_functionality_fixture():\n",
        "    # Convert Pandas DataFrame to Spark DataFrame\n",
        "    global expected_total_sold_products_spark_df\n",
        "    expected_total_sold_products_spark_df = convert_from_dict_to_spark(expected_total_sold_products_df_dict)\n",
        "    assert expected_total_sold_products_spark_df is not None, \\\n",
        "          'Error when created the expected DataFrame'\n",
        "\n",
        "@mark.parametrize('total_sold_per_caja', expected_total_sold_products)\n",
        "def test_create_spark_data_frame_sales_per_caja_row(total_sold_per_caja):\n",
        "    total_sold_spark_row = create_spark_data_frame_sales_per_caja_row(total_sold_per_caja[\"total_vendido\"],\n",
        "                                                                      total_sold_per_caja[\"caja\"])\n",
        "    assert total_sold_spark_row is not None\n",
        "    assert isinstance(total_sold_spark_row, Row)\n",
        "    assert [\"caja\", \"total_vendido\"] == list(total_sold_spark_row.asDict())\n",
        "    assert total_sold_per_caja[\"caja\"] == total_sold_spark_row[\"caja\"]\n",
        "    assert total_sold_per_caja[\"total_vendido\"] == total_sold_spark_row[\"total_vendido\"]\n",
        "\n",
        "def test_set_total_sold_per_caja_spark_data_frame_row():\n",
        "  sales_to_test = expected_products_sales_per_caja[0][\"compras\"]\n",
        "  total_sold_spark_row = set_total_sold_per_caja_spark_data_frame_row(sales_to_test)\n",
        "  expected_caja = 1\n",
        "\n",
        "  assert total_sold_spark_row is not None\n",
        "  assert isinstance(total_sold_spark_row, Row)\n",
        "  assert [\"caja\", \"total_vendido\"] == list(total_sold_spark_row.asDict())\n",
        "  assert str(expected_caja) == total_sold_spark_row[\"caja\"]\n",
        "\n",
        "def test_create_spark_data_frame_from_total_sold_rows_list(total_sold_products_functionality_fixture):\n",
        "  sales_list_to_test = [list(expected_products_sales_per_caja[0][\"compras\"])]\n",
        "  total_sold_df = create_spark_data_frame_from_total_sold_rows_list(sales_list_to_test)\n",
        "  \n",
        "  # Get Total Sold Product DataFrame from list\n",
        "  dataframes_difference = total_sold_df.exceptAll(expected_total_sold_products_spark_df)\n",
        "\n",
        "  # Check the resulting DataFrame from the difference has no rows as there\n",
        "  # is no difference between DataFrames\n",
        "  assert dataframes_difference.count() == 0\n",
        "\n",
        "def test_calculate_sold_products_per_caja(total_sold_products_functionality_fixture):\n",
        "  sales_list_to_test = [list(expected_products_sales_per_caja[0][\"compras\"])]\n",
        "  total_sold_df = calculate_sold_products_per_caja(sales_list_to_test)\n",
        "\n",
        "  # Get Total Sold Product DataFrame from list\n",
        "  dataframes_difference = total_sold_df.exceptAll(expected_total_sold_products_spark_df)\n",
        "\n",
        "  # Check the resulting DataFrame from the difference has no rows as there\n",
        "  # is no difference between DataFrames\n",
        "  assert dataframes_difference.count() == 0\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================= test session starts ==============================\n",
            "platform linux -- Python 3.6.8, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "rootdir: /usr/local, inifile: setup.cfg\n",
            "collected 5 items\n",
            "\n",
            "../usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py ....+----+-------------+\n",
            "|caja|total_vendido|\n",
            "+----+-------------+\n",
            "|1   |13464        |\n",
            "+----+-------------+\n",
            "\n",
            ".\n",
            "\n",
            "=========================== 5 passed in 4.33 seconds ===========================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7OyQ3lwnEPw",
        "colab_type": "text"
      },
      "source": [
        "**6.6) Pruebas unitarias para obtener las métricas**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DYbz2y0nODS",
        "colab_type": "code",
        "outputId": "4b9c4bc0-c827-41a4-f9c2-274403c2dc9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# This command is needed to run the UTs in Google Colab\n",
        "%%run_pytest[clean] -s\n",
        "\n",
        "@fixture(scope=\"module\")\n",
        "def metrics_functionality_fixture():\n",
        "    # Convert Pandas DataFrame to Spark DataFrame\n",
        "    global total_products_spark_df\n",
        "    total_products_spark_df = convert_from_dict_to_spark(expected_total_products_df_dict)\n",
        "    assert total_products_spark_df is not None, \\\n",
        "          'Error when created the expected DataFrame'\n",
        "    \n",
        "    global total_sold_products_spark_df\n",
        "    total_sold_products_spark_df = convert_from_dict_to_spark(expected_total_sold_products_df_dict)\n",
        "    assert total_sold_products_spark_df is not None, \\\n",
        "          'Error when created the expected DataFrame'\n",
        "\n",
        "@mark.parametrize('sales_metric', expected_sales_metrics)\n",
        "def test_create_spark_data_frame_metric_row(sales_metric):\n",
        "    sales_metric_spark_row = create_spark_data_frame_metric_row(sales_metric[\"tipo_metrica\"],\n",
        "                                                                sales_metric[\"valor\"])\n",
        "    assert sales_metric_spark_row is not None\n",
        "    assert isinstance(sales_metric_spark_row, Row)\n",
        "    assert [\"tipo_metrica\", \"valor\"] == list(sales_metric_spark_row.asDict())\n",
        "    assert sales_metric[\"tipo_metrica\"] == sales_metric_spark_row[\"tipo_metrica\"]\n",
        "    assert sales_metric[\"valor\"] == sales_metric_spark_row[\"valor\"]\n",
        "\n",
        "def test_get_caja_with_more_sales(metrics_functionality_fixture):\n",
        "  more_sales_metric_row = get_caja_with_more_sales(total_sold_products_spark_df)\n",
        "  assert more_sales_metric_row is not None\n",
        "  assert isinstance(more_sales_metric_row, Row)\n",
        "  assert [\"tipo_metrica\", \"valor\"] == list(more_sales_metric_row.asDict())\n",
        "  assert \"caja_con_mas_ventas\" == more_sales_metric_row[\"tipo_metrica\"]\n",
        "  assert \"1\" == more_sales_metric_row[\"valor\"]\n",
        "\n",
        "def test_get_caja_with_less_sales(metrics_functionality_fixture):\n",
        "  less_sales_metric_row = get_caja_with_less_sales(total_sold_products_spark_df)\n",
        "  assert less_sales_metric_row is not None\n",
        "  assert isinstance(less_sales_metric_row, Row)\n",
        "  assert [\"tipo_metrica\", \"valor\"] == list(less_sales_metric_row.asDict())\n",
        "  assert \"caja_con_menos_ventas\" == less_sales_metric_row[\"tipo_metrica\"]\n",
        "  assert \"1\" == less_sales_metric_row[\"valor\"]\n",
        "\n",
        "def test_get_product_with_more_sales(metrics_functionality_fixture):\n",
        "  product_with_more_sales_metric_row = get_product_with_more_sales(total_products_spark_df)\n",
        "  assert product_with_more_sales_metric_row is not None\n",
        "  assert isinstance(product_with_more_sales_metric_row, Row)\n",
        "  assert [\"tipo_metrica\", \"valor\"] == list(product_with_more_sales_metric_row.asDict())\n",
        "  assert \"producto_mas_vendido_por_unidad\" == product_with_more_sales_metric_row[\"tipo_metrica\"]\n",
        "  assert \"Chocolate\" == product_with_more_sales_metric_row[\"valor\"]\n",
        "\n",
        "def test_get_higher_income_product():\n",
        "  sales_list_to_test = [list(expected_products_sales_per_caja[0][\"compras\"])]\n",
        "  higher_income_product_metric_row = get_higher_income_product(sales_list_to_test)\n",
        "  assert higher_income_product_metric_row is not None\n",
        "  assert isinstance(higher_income_product_metric_row, Row)\n",
        "  assert [\"tipo_metrica\", \"valor\"] == list(higher_income_product_metric_row.asDict())\n",
        "  assert \"producto_de_mayor_ingreso\" == higher_income_product_metric_row[\"tipo_metrica\"]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================= test session starts ==============================\n",
            "platform linux -- Python 3.6.8, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "rootdir: /usr/local, inifile: setup.cfg\n",
            "collected 11 items\n",
            "\n",
            "../usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py ...........\n",
            "\n",
            "========================== 11 passed in 1.02 seconds ===========================\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}